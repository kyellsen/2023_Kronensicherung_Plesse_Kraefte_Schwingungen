{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"2023_Kronensicherung_Plesse_Kraefte_Schwingungen\"\n",
    "author: \"Kyell Jensen\"\n",
    "date: \"2024-08-06\"\n",
    "format: pdf\n",
    "editor: visual\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2023_Kronensicherung_Plesse_Kraefte_Schwingungen\n",
    "\n",
    "## Kombinierte Analyse LineScale3, TreeQinetic und Versuchsaufzeichung\n",
    "\n",
    "Nutze eine geeignete Python 3.11 Umgebung (z. B. virtuelle Environment) und installiere die Pakete linescale3 (LS3) und treeqinetic (PTQ) inklusive kj_core und kj_logger und weiteren requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Package Importe\n",
    "\n",
    "Es werden zuerst benötigte Standard-Pakete importiert. Nachfolgend die zwei extra geschriebenen Pakete LS3 und PTQ. Fehler beim Import dieser zwei Pakete sind ggf. Bugs. Beide Pakete nutzen eine gemeinsame CodeBasis in den Paketen kj_core (Core-Package) und kj_logger (individualisiertes Logging des Verarbeitungs-Prozesses). Diese sollte i. d. R. über die requirements mit installiert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Import von Standardbibliotheken\n",
    "\n",
    "Die folgenden Bibliotheken werden importiert, um grundlegende Funktionen für Strukturierung, Datenverarbeitung, Plotting und statistische Auswertung bereit zu stellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T10:25:20.664404400Z",
     "start_time": "2025-03-23T10:25:19.962778900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Struktur\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "# Datenverarbeitung\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from slugify import slugify  # Slugify ums strings in standard Formate zu überführen\n",
    "\n",
    "# Plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Statistik\n",
    "from scipy.stats import linregress, f_oneway\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats.multicomp as mc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Import TreeQinetic\n",
    "\n",
    "Das Packet TreeQinetic wurde vom Autor (Kyell Jensen) zum einfachen Analysieren, Plotten und zur Interpretation der TXT-Messdaten der Picus TreeQinetic Elastometer und Inclinometer der Firma IML Instrumenta Mechanik Labor Electronic GmbH geschrieben (https://www.iml-electronic.de/produkt/picus-treeqinetic/). Nachfolgend wird das Packet und einige dort definierten Klassen importiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-23T10:25:21.312615500Z",
     "start_time": "2025-03-23T10:25:20.000690500Z"
    }
   },
   "outputs": [],
   "source": [
    "import treeqinetic as ptq\n",
    "# ptq.help() # Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Daten Import und explorative Datenanalyse\n",
    "\n",
    "Lege Pfade für Daten-Importe, Daten-Exporte etc. fest (ggf. anpassen an eigene Verzeichnisstruktur), ausgelagert in gemeinsame Config für verschiedene Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-23T10:25:21.350407800Z",
     "start_time": "2025-03-23T10:25:20.021280Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importiere alle Einstellungen aus der project_config.py\n",
    "from project_config import (\n",
    "    main_path,\n",
    "    analyse_name,\n",
    "    data_path,\n",
    "    working_directory,\n",
    "    data_export_directory,\n",
    "    latex_export_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### METADATA: Lade Metadaten zum Versuch (Baumdaten, Geräteanordnung, Versuchsablauf)\n",
    "\n",
    "Die Daten wurden während des Versuchs als GoogleSheet erfasst und abschließend als CSV-Dateien exportiert, um ein einfaches Einlesen zur ermöglichen.\n",
    "Nachfolgend werden Datentypen für alle Spalten explizit in den *_data_dict.json angegeben, da es teils durch Python-Pandas zu Fehlerkennungen kommt.\n",
    "Zusätzlich werden teils Spaltennamen vereinheitlicht oder benannt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Daten des Versuchsbaums (tree_df)\n",
    "\n",
    "Die Tabelle `tree_df` liefert spezifische Informationen über den Versuchsbaum. Da nur ein Versuchsbaum, sind hier Daten und Datendokumentation in einer Datei."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-23T10:25:21.428533900Z",
     "start_time": "2025-03-23T10:25:20.062959200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "               Variable Kategorie                 Zeichen  \\\n0               baumart      tree                 Baumart   \n1             belaubung      tree               Belaubung   \n2            vitalitaet      tree              Vitalitaet   \n3                h_st_a      tree      $h_{\\mathrm{StA}}$   \n4                h_st_b      tree      $h_{\\mathrm{StB}}$   \n5                  u_1m      tree     $u_{1\\,\\mathrm{m}}$   \n6             h_zwiesel      tree  $h_{\\mathrm{Zwiesel}}$   \n7                  h_ks      tree       $h_{\\mathrm{KS}}$   \n8                  l_ks      tree       $l_{\\mathrm{KS}}$   \n9           u_st_a_h_ks      tree   $u_{\\mathrm{StA,KS}}$   \n10          u_st_b_h_ks      tree   $u_{\\mathrm{StB,KS}}$   \n11        standort_h_nn      tree       $h_{\\mathrm{NN}}$   \n12  standort_geo_breite      tree               $\\varphi$   \n13  standort_geo_laenge      tree               $\\lambda$   \n\n                           Deutsch Datentyp Einheit  \\\n0                          Baumart   string       -   \n1                        Belaubung   string       -   \n2         Vitalität nach A. Roloff    Int64   Stufe   \n3                 Höhe Stämmling A  float64       m   \n4                 Höhe Stämmling B  float64       m   \n5                   Umfang auf 1~m  float64      cm   \n6                     Höhe Zwiesel  float64       m   \n7             Höhe Kronensicherung  float64       m   \n8            Länge Kronensicherung  float64      cm   \n9   Umfang Stämmling A auf Höhe KS  float64      cm   \n10  Umfang Stämmling B auf Höhe KS  float64      cm   \n11        Standort Höhe über n. N.  float64       m   \n12            Standort Geo. Breite  float64    Grad   \n13             Standort Geo. Länge  float64    Grad   \n\n                           Beschreibung             Wert  \n0                        Art des Baumes  Fagus silvatica  \n1         Belaubungszustand zur Messung        unbelaubt  \n2   Vitalitätsstufe nach Andreas Roloff                1  \n3           Gesamthöhe des Stämmlings A            26.15  \n4           Gesamthöhe des Stämmlings B            27.20  \n5              Stammumfang auf 1~m Höhe           140.00  \n6          Höhe des Zwiesels über Boden            10.31  \n7     Höhe der KS-Anbringung über Boden            17.40  \n8              Länge der eingebauten KS           135.00  \n9     Stammumfang von A auf Höhe der KS            58.00  \n10    Stammumfang von B auf Höhe der KS            48.00  \n11         Standorthöhe über Normalnull           352.00  \n12    Geografische Breite des Standorts        51.589476  \n13     Geografische Länge des Standorts         9.985242  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Variable</th>\n      <th>Kategorie</th>\n      <th>Zeichen</th>\n      <th>Deutsch</th>\n      <th>Datentyp</th>\n      <th>Einheit</th>\n      <th>Beschreibung</th>\n      <th>Wert</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>baumart</td>\n      <td>tree</td>\n      <td>Baumart</td>\n      <td>Baumart</td>\n      <td>string</td>\n      <td>-</td>\n      <td>Art des Baumes</td>\n      <td>Fagus silvatica</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>belaubung</td>\n      <td>tree</td>\n      <td>Belaubung</td>\n      <td>Belaubung</td>\n      <td>string</td>\n      <td>-</td>\n      <td>Belaubungszustand zur Messung</td>\n      <td>unbelaubt</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>vitalitaet</td>\n      <td>tree</td>\n      <td>Vitalitaet</td>\n      <td>Vitalität nach A. Roloff</td>\n      <td>Int64</td>\n      <td>Stufe</td>\n      <td>Vitalitätsstufe nach Andreas Roloff</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>h_st_a</td>\n      <td>tree</td>\n      <td>$h_{\\mathrm{StA}}$</td>\n      <td>Höhe Stämmling A</td>\n      <td>float64</td>\n      <td>m</td>\n      <td>Gesamthöhe des Stämmlings A</td>\n      <td>26.15</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>h_st_b</td>\n      <td>tree</td>\n      <td>$h_{\\mathrm{StB}}$</td>\n      <td>Höhe Stämmling B</td>\n      <td>float64</td>\n      <td>m</td>\n      <td>Gesamthöhe des Stämmlings B</td>\n      <td>27.20</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>u_1m</td>\n      <td>tree</td>\n      <td>$u_{1\\,\\mathrm{m}}$</td>\n      <td>Umfang auf 1~m</td>\n      <td>float64</td>\n      <td>cm</td>\n      <td>Stammumfang auf 1~m Höhe</td>\n      <td>140.00</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>h_zwiesel</td>\n      <td>tree</td>\n      <td>$h_{\\mathrm{Zwiesel}}$</td>\n      <td>Höhe Zwiesel</td>\n      <td>float64</td>\n      <td>m</td>\n      <td>Höhe des Zwiesels über Boden</td>\n      <td>10.31</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>h_ks</td>\n      <td>tree</td>\n      <td>$h_{\\mathrm{KS}}$</td>\n      <td>Höhe Kronensicherung</td>\n      <td>float64</td>\n      <td>m</td>\n      <td>Höhe der KS-Anbringung über Boden</td>\n      <td>17.40</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>l_ks</td>\n      <td>tree</td>\n      <td>$l_{\\mathrm{KS}}$</td>\n      <td>Länge Kronensicherung</td>\n      <td>float64</td>\n      <td>cm</td>\n      <td>Länge der eingebauten KS</td>\n      <td>135.00</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>u_st_a_h_ks</td>\n      <td>tree</td>\n      <td>$u_{\\mathrm{StA,KS}}$</td>\n      <td>Umfang Stämmling A auf Höhe KS</td>\n      <td>float64</td>\n      <td>cm</td>\n      <td>Stammumfang von A auf Höhe der KS</td>\n      <td>58.00</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>u_st_b_h_ks</td>\n      <td>tree</td>\n      <td>$u_{\\mathrm{StB,KS}}$</td>\n      <td>Umfang Stämmling B auf Höhe KS</td>\n      <td>float64</td>\n      <td>cm</td>\n      <td>Stammumfang von B auf Höhe der KS</td>\n      <td>48.00</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>standort_h_nn</td>\n      <td>tree</td>\n      <td>$h_{\\mathrm{NN}}$</td>\n      <td>Standort Höhe über n. N.</td>\n      <td>float64</td>\n      <td>m</td>\n      <td>Standorthöhe über Normalnull</td>\n      <td>352.00</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>standort_geo_breite</td>\n      <td>tree</td>\n      <td>$\\varphi$</td>\n      <td>Standort Geo. Breite</td>\n      <td>float64</td>\n      <td>Grad</td>\n      <td>Geografische Breite des Standorts</td>\n      <td>51.589476</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>standort_geo_laenge</td>\n      <td>tree</td>\n      <td>$\\lambda$</td>\n      <td>Standort Geo. Länge</td>\n      <td>float64</td>\n      <td>Grad</td>\n      <td>Geografische Länge des Standorts</td>\n      <td>9.985242</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype_dict = {\n",
    "    'Attribut': 'string',\n",
    "    'Wert': 'string',\n",
    "    'Einheit': 'string'\n",
    "}\n",
    "tree_file = data_path / 'tree.csv'\n",
    "tree_df = pd.read_csv(tree_file,\n",
    "                      sep=';', decimal=',', na_values='NA',\n",
    "                      dtype=dtype_dict)\n",
    "tree_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "               Variable Kategorie                 Zeichen  \\\n0               baumart      tree                 Baumart   \n1             belaubung      tree               Belaubung   \n2            vitalitaet      tree              Vitalitaet   \n3                h_st_a      tree      $h_{\\mathrm{StA}}$   \n4                h_st_b      tree      $h_{\\mathrm{StB}}$   \n5                  u_1m      tree     $u_{1\\,\\mathrm{m}}$   \n6             h_zwiesel      tree  $h_{\\mathrm{Zwiesel}}$   \n7                  h_ks      tree       $h_{\\mathrm{KS}}$   \n8                  l_ks      tree       $l_{\\mathrm{KS}}$   \n9           u_st_a_h_ks      tree   $u_{\\mathrm{StA,KS}}$   \n10          u_st_b_h_ks      tree   $u_{\\mathrm{StB,KS}}$   \n11        standort_h_nn      tree       $h_{\\mathrm{NN}}$   \n12  standort_geo_breite      tree               $\\varphi$   \n13  standort_geo_laenge      tree               $\\lambda$   \n\n                           Deutsch Datentyp Einheit  \\\n0                          Baumart   string       -   \n1                        Belaubung   string       -   \n2         Vitalität nach A. Roloff    Int64   Stufe   \n3                 Höhe Stämmling A  float64       m   \n4                 Höhe Stämmling B  float64       m   \n5                   Umfang auf 1~m  float64      cm   \n6                     Höhe Zwiesel  float64       m   \n7             Höhe Kronensicherung  float64       m   \n8            Länge Kronensicherung  float64      cm   \n9   Umfang Stämmling A auf Höhe KS  float64      cm   \n10  Umfang Stämmling B auf Höhe KS  float64      cm   \n11        Standort Höhe über n. N.  float64       m   \n12            Standort Geo. Breite  float64    Grad   \n13             Standort Geo. Länge  float64    Grad   \n\n                           Beschreibung             Wert  \n0                        Art des Baumes  Fagus silvatica  \n1         Belaubungszustand zur Messung        unbelaubt  \n2   Vitalitätsstufe nach Andreas Roloff                1  \n3           Gesamthöhe des Stämmlings A            26.15  \n4           Gesamthöhe des Stämmlings B            27.20  \n5              Stammumfang auf 1~m Höhe           140.00  \n6          Höhe des Zwiesels über Boden            10.31  \n7     Höhe der KS-Anbringung über Boden            17.40  \n8              Länge der eingebauten KS           135.00  \n9     Stammumfang von A auf Höhe der KS            58.00  \n10    Stammumfang von B auf Höhe der KS            48.00  \n11         Standorthöhe über Normalnull           352.00  \n12    Geografische Breite des Standorts        51.589476  \n13     Geografische Länge des Standorts         9.985242  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Variable</th>\n      <th>Kategorie</th>\n      <th>Zeichen</th>\n      <th>Deutsch</th>\n      <th>Datentyp</th>\n      <th>Einheit</th>\n      <th>Beschreibung</th>\n      <th>Wert</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>baumart</td>\n      <td>tree</td>\n      <td>Baumart</td>\n      <td>Baumart</td>\n      <td>string</td>\n      <td>-</td>\n      <td>Art des Baumes</td>\n      <td>Fagus silvatica</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>belaubung</td>\n      <td>tree</td>\n      <td>Belaubung</td>\n      <td>Belaubung</td>\n      <td>string</td>\n      <td>-</td>\n      <td>Belaubungszustand zur Messung</td>\n      <td>unbelaubt</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>vitalitaet</td>\n      <td>tree</td>\n      <td>Vitalitaet</td>\n      <td>Vitalität nach A. Roloff</td>\n      <td>Int64</td>\n      <td>Stufe</td>\n      <td>Vitalitätsstufe nach Andreas Roloff</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>h_st_a</td>\n      <td>tree</td>\n      <td>$h_{\\mathrm{StA}}$</td>\n      <td>Höhe Stämmling A</td>\n      <td>float64</td>\n      <td>m</td>\n      <td>Gesamthöhe des Stämmlings A</td>\n      <td>26.15</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>h_st_b</td>\n      <td>tree</td>\n      <td>$h_{\\mathrm{StB}}$</td>\n      <td>Höhe Stämmling B</td>\n      <td>float64</td>\n      <td>m</td>\n      <td>Gesamthöhe des Stämmlings B</td>\n      <td>27.20</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>u_1m</td>\n      <td>tree</td>\n      <td>$u_{1\\,\\mathrm{m}}$</td>\n      <td>Umfang auf 1~m</td>\n      <td>float64</td>\n      <td>cm</td>\n      <td>Stammumfang auf 1~m Höhe</td>\n      <td>140.00</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>h_zwiesel</td>\n      <td>tree</td>\n      <td>$h_{\\mathrm{Zwiesel}}$</td>\n      <td>Höhe Zwiesel</td>\n      <td>float64</td>\n      <td>m</td>\n      <td>Höhe des Zwiesels über Boden</td>\n      <td>10.31</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>h_ks</td>\n      <td>tree</td>\n      <td>$h_{\\mathrm{KS}}$</td>\n      <td>Höhe Kronensicherung</td>\n      <td>float64</td>\n      <td>m</td>\n      <td>Höhe der KS-Anbringung über Boden</td>\n      <td>17.40</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>l_ks</td>\n      <td>tree</td>\n      <td>$l_{\\mathrm{KS}}$</td>\n      <td>Länge Kronensicherung</td>\n      <td>float64</td>\n      <td>cm</td>\n      <td>Länge der eingebauten KS</td>\n      <td>135.00</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>u_st_a_h_ks</td>\n      <td>tree</td>\n      <td>$u_{\\mathrm{StA,KS}}$</td>\n      <td>Umfang Stämmling A auf Höhe KS</td>\n      <td>float64</td>\n      <td>cm</td>\n      <td>Stammumfang von A auf Höhe der KS</td>\n      <td>58.00</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>u_st_b_h_ks</td>\n      <td>tree</td>\n      <td>$u_{\\mathrm{StB,KS}}$</td>\n      <td>Umfang Stämmling B auf Höhe KS</td>\n      <td>float64</td>\n      <td>cm</td>\n      <td>Stammumfang von B auf Höhe der KS</td>\n      <td>48.00</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>standort_h_nn</td>\n      <td>tree</td>\n      <td>$h_{\\mathrm{NN}}$</td>\n      <td>Standort Höhe über n. N.</td>\n      <td>float64</td>\n      <td>m</td>\n      <td>Standorthöhe über Normalnull</td>\n      <td>352.00</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>standort_geo_breite</td>\n      <td>tree</td>\n      <td>$\\varphi$</td>\n      <td>Standort Geo. Breite</td>\n      <td>float64</td>\n      <td>Grad</td>\n      <td>Geografische Breite des Standorts</td>\n      <td>51.589476</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>standort_geo_laenge</td>\n      <td>tree</td>\n      <td>$\\lambda$</td>\n      <td>Standort Geo. Länge</td>\n      <td>float64</td>\n      <td>Grad</td>\n      <td>Geografische Länge des Standorts</td>\n      <td>9.985242</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alle Spalten pauschal als string lesen\n",
    "tree_df = pd.read_csv(\n",
    "    data_path / 'tree.csv',\n",
    "    sep=';', decimal=',',\n",
    "    na_values='NA',\n",
    "    dtype='string'  # alle Spalten als string\n",
    ")\n",
    "\n",
    "tree_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-23T10:25:21.518571300Z",
     "start_time": "2025-03-23T10:25:20.074856300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Daten der Geräteanordnung am Baum (sensor_df)\n",
    "\n",
    "Die `sensor_position.csv` enthält detaillierte Informationen zur Anordnung der Sensoren an den Bäumen im Rahmen des Experiments. Jede Zeile beschreibt die Platzierung eines Sensors. Die Datendokumentation ergibt sich aus `sensor_position_data_dict.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-23T10:25:21.609909200Z",
     "start_time": "2025-03-23T10:25:20.088305100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   position_id  type sensor_id location  height  circumference direction  \\\n0            1   LS3  14:BF:E6     rope    18.3           <NA>      <NA>   \n1            2   LS3  14:99:1E    cable    17.4           <NA>      <NA>   \n2            3  TMS1       015      StA    18.0           0.45      west   \n3            4  TMS1       014      StB    18.0            0.4      west   \n4            5  TMS1       013      StA    15.0           0.67      west   \n\n   note  diameter  \n0  <NA>      <NA>  \n1  <NA>      <NA>  \n2  <NA>  0.143239  \n3  <NA>  0.127324  \n4  <NA>  0.213268  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>position_id</th>\n      <th>type</th>\n      <th>sensor_id</th>\n      <th>location</th>\n      <th>height</th>\n      <th>circumference</th>\n      <th>direction</th>\n      <th>note</th>\n      <th>diameter</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>LS3</td>\n      <td>14:BF:E6</td>\n      <td>rope</td>\n      <td>18.3</td>\n      <td>&lt;NA&gt;</td>\n      <td>&lt;NA&gt;</td>\n      <td>&lt;NA&gt;</td>\n      <td>&lt;NA&gt;</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>LS3</td>\n      <td>14:99:1E</td>\n      <td>cable</td>\n      <td>17.4</td>\n      <td>&lt;NA&gt;</td>\n      <td>&lt;NA&gt;</td>\n      <td>&lt;NA&gt;</td>\n      <td>&lt;NA&gt;</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>TMS1</td>\n      <td>015</td>\n      <td>StA</td>\n      <td>18.0</td>\n      <td>0.45</td>\n      <td>west</td>\n      <td>&lt;NA&gt;</td>\n      <td>0.143239</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>TMS1</td>\n      <td>014</td>\n      <td>StB</td>\n      <td>18.0</td>\n      <td>0.4</td>\n      <td>west</td>\n      <td>&lt;NA&gt;</td>\n      <td>0.127324</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>TMS1</td>\n      <td>013</td>\n      <td>StA</td>\n      <td>15.0</td>\n      <td>0.67</td>\n      <td>west</td>\n      <td>&lt;NA&gt;</td>\n      <td>0.213268</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lade das Dictionary mit der Daten Dokumentation\n",
    "with open(data_path/ \"sensor_position_data_dict.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    sensor_position_dict = json.load(f)\n",
    "\n",
    "# Erzeuge dtype_dict dynamisch aus dem der Daten Dokumentation\n",
    "dtype_dict = {key: value[\"Datentyp\"] for key, value in sensor_position_dict.items()\n",
    "    if value[\"Datentyp\"] not in [None, \"\"]\n",
    "}\n",
    "\n",
    "# CSV einlesen mit dynamischen Datentypen\n",
    "sensor_position_file = data_path / 'sensor_position.csv'\n",
    "sensor_df = pd.read_csv(sensor_position_file, sep=';', decimal=',', na_values='NA',dtype=dtype_dict)\n",
    "\n",
    "sensor_df[\"height\"] = sensor_df[\"height\"] / 100\n",
    "sensor_df[\"circumference\"] = sensor_df[\"circumference\"] / 100\n",
    "sensor_df[\"diameter\"] = sensor_df[\"circumference\"] / np.pi\n",
    "\n",
    "sensor_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-23T10:25:21.609909200Z",
     "start_time": "2025-03-23T10:25:20.106889600Z"
    }
   },
   "outputs": [],
   "source": [
    "#sensor_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Daten des Versuchsablaufs (series_df)\n",
    "\n",
    "Die `series.csv` enthält das Protokoll des Versuchsablaufes der einzelnen Messungen. Jede Zeile repräsentiert eine einzelne Messung. Die Datendokumentation ergibt sich aus `series_data_dict.py`.\n",
    "\n",
    "##### Behandlungsvariante/Kronensicherung Kategorien\n",
    "\n",
    "- **`free`**: Der Baum konnte frei ohne Kronensicherung nach dem Release ausschwingen.\n",
    "  \n",
    "- **`gefa_dynamic`**: In ca. 2/3 der Baumhöhe wurde ein dynamisches Gefa Gurtband 4t dynamisch nach ZTV-Baumpflege mit leichtem Durchhang installiert. Das Ausschwingen wurde durch die KS abgedämpft. Da die Sicherung ohne Vorspannung installiert wurde, zeigen die Plots (/ls3/plots/force_vs_time_1/) von '14:99:1E' sowohl am Anfang als auch am Ende ca. 0 kN Kraft an (LogNr 1 bis 9).\n",
    "\n",
    "- **`cobra_static`**: In ca. 2/3 der Baumhöhe wurde eine statische Cobra ultrastatic 7t (Dyneema) Sicherung installiert. Die Vorspannung betrug ca. 0,4 kN, wie sich in den Plots (/ls3/plots/force_vs_time_1/) von '14:99:1E' gut erkennen lässt. Durch das Zusammenziehen der Stämmlinge ist die Kronensicherung vor dem Release vollständig lastfrei (0 kN). Nach dem Release pendelt sich die Kraft ca. bei 0,4 kN ein (LogNr 10-18).\n",
    "\n",
    "- **`cobra_static_slack`**: Ähnlich der `cobra_static`, jedoch wurde die Vorspannung entfernt. Aufgrund von Regen wurde nur eine Messung durchgeführt und die Serie frühzeitig abgebrochen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-23T10:25:21.609909200Z",
     "start_time": "2025-03-23T10:25:20.110897Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   id      time  release_force_target treatment  14:BF:E6  14:99:1E\n0   1  10:10:00                   2.5      free         1      <NA>\n1   2  10:20:00                   2.8      free         2      <NA>\n2   3  10:28:00                   2.8      free         3      <NA>\n3   4  10:47:00                   2.8      free         4      <NA>\n4   5  10:53:00                   2.4      free         5      <NA>",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>time</th>\n      <th>release_force_target</th>\n      <th>treatment</th>\n      <th>14:BF:E6</th>\n      <th>14:99:1E</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>10:10:00</td>\n      <td>2.5</td>\n      <td>free</td>\n      <td>1</td>\n      <td>&lt;NA&gt;</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>10:20:00</td>\n      <td>2.8</td>\n      <td>free</td>\n      <td>2</td>\n      <td>&lt;NA&gt;</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>10:28:00</td>\n      <td>2.8</td>\n      <td>free</td>\n      <td>3</td>\n      <td>&lt;NA&gt;</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>10:47:00</td>\n      <td>2.8</td>\n      <td>free</td>\n      <td>4</td>\n      <td>&lt;NA&gt;</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>10:53:00</td>\n      <td>2.4</td>\n      <td>free</td>\n      <td>5</td>\n      <td>&lt;NA&gt;</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lade das Dictionary mit der Daten Dokumentation\n",
    "with open(data_path / \"series_data_dict.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    series_dict = json.load(f)\n",
    "\n",
    "# Erzeuge dtype_dict dynamisch aus der Daten Dokumentation\n",
    "dtype_dict = {key: value[\"Datentyp\"] for key, value in series_dict.items()\n",
    "              if value[\"Datentyp\"] not in [None, \"\"]\n",
    "}\n",
    "\n",
    "# CSV einlesen mit dynamischen Datentypen\n",
    "series_file = data_path / 'series.csv'\n",
    "series_df = pd.read_csv(series_file, sep=';', decimal=',', na_values='NA', dtype=dtype_dict)\n",
    "\n",
    "# Zeitspalte in Uhrzeit umwandeln\n",
    "series_df[\"time\"] = pd.to_datetime(series_df[\"time\"], format=\"%H:%M:%S\").dt.time\n",
    "\n",
    "# Behandlungskategorien sortiert definieren\n",
    "treatment_order = ['free', 'gefa_dynamic', 'cobra_static', 'cobra_static_slack']\n",
    "treatment_category = CategoricalDtype(categories=treatment_order, ordered=True)\n",
    "series_df[\"treatment\"] = series_df[\"treatment\"].astype(treatment_category)\n",
    "\n",
    "# Vorschau\n",
    "series_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-23T10:25:21.609909200Z",
     "start_time": "2025-03-23T10:25:20.131921600Z"
    }
   },
   "outputs": [],
   "source": [
    "#series_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Latex Export: Daten und Datendokumentation als Latex-Tabellen\n",
    "\n",
    "Definiere eine allgemeine Export-Funktion, um die Tabellen als eigene Dateien zu speichern und in Latex zu importieren"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "from utils_latex_export import (\n",
    "    save_latex_table,\n",
    "    extract_latex_dict_from_json\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-23T10:25:21.660516400Z",
     "start_time": "2025-03-23T10:25:20.132916800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Latex Export: Daten des Versuchsbaums (tree_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Daten und Datendokumentation in einem:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content saved to: C:\\kyellsen\\005_Projekte\\2024_BA\\032_Feldversuch_2023_Plesse\\030_Analysen\\2023_Kronensicherung_Plesse_Kraefte_Schwingungen\\working_directory\\export_latex\\feldversuch_2_daten_des_versuchsbaums.tex\n"
     ]
    }
   ],
   "source": [
    "select_data_fields = [\"Zeichen\", \"Deutsch\", \"Wert\", \"Datentyp\", \"Einheit\"]\n",
    "tree_df_latex = tree_df[select_data_fields].copy()\n",
    "\n",
    "latex_string = tree_df_latex.to_latex(index=False, escape=False, column_format=\"llrrr\", \n",
    "                                      float_format=\"{:0.2f}\".format)\n",
    "caption = \"Feldversuch 2 - Daten des Versuchsbaums\"\n",
    "\n",
    "save_latex_table(latex_string, caption, latex_export_directory)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-23T10:25:21.721377400Z",
     "start_time": "2025-03-23T10:25:20.174335300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Latex Export: Daten der Geräteanordnung am Baum (sensor_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "select_data_fields = [\"Variable\", \"Zeichen\", \"Deutsch\", \"Datentyp\", \"Einheit\", \"Beschreibung\"]\n",
    "sensor_select_variables = [\"type\", \"sensor_id\", \"location\", \"direction\", \"height\", \"circumference\", \"diameter\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-23T10:25:21.772201500Z",
     "start_time": "2025-03-23T10:25:20.204503400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Daten:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content saved to: C:\\kyellsen\\005_Projekte\\2024_BA\\032_Feldversuch_2023_Plesse\\030_Analysen\\2023_Kronensicherung_Plesse_Kraefte_Schwingungen\\working_directory\\export_latex\\feldversuch_2_gerateanordnung.tex\n"
     ]
    }
   ],
   "source": [
    "sensor_df_latex = sensor_df.copy()[sensor_select_variables]\n",
    "\n",
    "# Ersetze Spaltennamen durch Formelzeichen\n",
    "sensor_df_latex.columns = [sensor_position_dict[col][\"Zeichen\"] for col in sensor_select_variables]\n",
    "\n",
    "latex_string = sensor_df_latex.to_latex(index=False, escape=False, column_format=\"lllrrrr\",\n",
    "    float_format=\"{:0.2f}\".format)\n",
    "\n",
    "caption = \"Feldversuch 2 - Geräteanordnung\"\n",
    "save_latex_table(latex_string, caption, latex_export_directory)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-23T10:25:21.833237700Z",
     "start_time": "2025-03-23T10:25:20.232005300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Datendokumentation:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content saved to: C:\\kyellsen\\005_Projekte\\2024_BA\\032_Feldversuch_2023_Plesse\\030_Analysen\\2023_Kronensicherung_Plesse_Kraefte_Schwingungen\\working_directory\\export_latex\\feldversuch_2_gerateanordnung_daten_dokumentation.tex\n"
     ]
    }
   ],
   "source": [
    "sensor_data_dict_df = extract_latex_dict_from_json(sensor_position_dict, sensor_select_variables, select_data_fields)\n",
    "latex_string = sensor_data_dict_df.to_latex(index=False, escape=False)\n",
    "caption = \"Feldversuch 2 - Geräteanordnung Daten Dokumentation\"\n",
    "\n",
    "save_latex_table(latex_string, caption, latex_export_directory)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-23T10:25:21.894120600Z",
     "start_time": "2025-03-23T10:25:20.253293100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Latex Export: Daten des Versuchsablaufs (series_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "series_select_variables = [\"id\", \"time\", \"release_force_target\", \"treatment\", \"14:BF:E6\", \"14:99:1E\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-23T10:25:21.956878500Z",
     "start_time": "2025-03-23T10:25:20.286557800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Daten:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content saved to: C:\\kyellsen\\005_Projekte\\2024_BA\\032_Feldversuch_2023_Plesse\\030_Analysen\\2023_Kronensicherung_Plesse_Kraefte_Schwingungen\\working_directory\\export_latex\\feldversuch_2_versuchsablauf.tex\n"
     ]
    }
   ],
   "source": [
    "series_df_latex = series_df.copy()[series_select_variables]\n",
    "\n",
    "# Ersetze Spaltennamen durch Formelzeichen\n",
    "series_df_latex.columns = [series_dict[col][\"Zeichen\"] for col in series_select_variables]\n",
    "\n",
    "series_df_latex[\"treatment\"] = series_df_latex[\"treatment\"].apply(\n",
    "    lambda x: \"\\\\texttt{\" + str(x).replace(\"_\", \"\\\\_\") + \"}\"\n",
    ")\n",
    "\n",
    "latex_string = series_df_latex.to_latex(index=False, escape=False, column_format=\"llrlrr\",\n",
    "                                        float_format=\"{:0.2f}\".format)\n",
    "\n",
    "caption = \"Feldversuch 2 - Versuchsablauf\"\n",
    "caption_long = \"Feldversuch 2 - Versuchsablauf\"\n",
    "\n",
    "save_latex_table(latex_string, caption, latex_export_directory, caption_long=caption_long)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-23T10:25:21.992553Z",
     "start_time": "2025-03-23T10:25:20.325082Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Datendokumentation:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content saved to: C:\\kyellsen\\005_Projekte\\2024_BA\\032_Feldversuch_2023_Plesse\\030_Analysen\\2023_Kronensicherung_Plesse_Kraefte_Schwingungen\\working_directory\\export_latex\\feldversuch_2_versuchsablauf_daten_dokumentation.tex\n"
     ]
    }
   ],
   "source": [
    "series_data_dict_df = extract_latex_dict_from_json(series_dict, series_select_variables, select_data_fields)\n",
    "\n",
    "latex_string = series_data_dict_df.to_latex(index=False, escape=False)\n",
    "caption = \"Feldversuch 2 - Versuchsablauf Daten Dokumentation\"\n",
    "\n",
    "save_latex_table(latex_string, caption, latex_export_directory)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-23T10:25:22.034015800Z",
     "start_time": "2025-03-23T10:25:20.344561800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PTQ: Daten der Elastometer (PicusTreeQinetic Data = PTQ)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PTQ: Daten Import\n",
    "\n",
    "Aus dem Zugversuchsset wurden 4 Elastometer und 3 Inclinometer verwendet.\n",
    "\n",
    "Die Elastometer waren auf dem rechten und linken Stämmling auf der Außenseite in zwei Ebenen platziert. Beim Zusammenziehen der Stämmlinge messen diese entsprechnd eine Faserdehnung, beim Ausschwingen der Stämmlinge über ihre Ruhelage hinaus nach außen eine Faserstauchung.\n",
    "\n",
    "Die Inclinometer Daten werden hier ebefalls geladen, erschienen aber weniger geeignet zur Auswertung und werden entspricht nicht weiter berücksichtigt.\n",
    "\n",
    "Die Funktion 'ptq.setup' erstellt div. Instanzen, die für das Paket notwendig sind (CONFIG, LOG_MANAGER, PLOT_MANAGER). \n",
    "\n",
    "Über die Klasse 'ptq.Series' wird eine neue Messreihe initialisiert und als 'ptq_series' gespeichert. Im Verzeichnis ptq_data_path finden sich die PTQ Daten als TXT von insgesamt 29 Messungen. Eine Datei enthält jeweils die Daten für alle Inclinometer und Elastometer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ptq.setup(working_directory=working_directory, log_level=\"info\", safe_logs_to_file=True)\n",
    "\n",
    "ptq_data_path = data_path / 'PTQ/data_txt'\n",
    "ptq_series = ptq.classes.Series(name=analyse_name, path=ptq_data_path)\n",
    "\n",
    "elasto_names = [\"Elasto(95)\", \"Elasto(98)\", \"Elasto(92)\", \"Elasto(90)\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PTQ: Explorative Datenanalyse\n",
    "\n",
    "Übersicht über alle vom PTQ erfassten Daten über alle Messungen gemeinsam (Elastos und Inclinos)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ptq_df = ptq_series.get_measurements_df()\n",
    "ptq_df.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### PTQ: Einhaltung der maximalen Faserdehnung"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ptq_df_elasto_summary = ptq_df[elasto_names].describe()\n",
    "ptq_df_elasto_summary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Analyse maximaler Dehnung und Stauchung, Prüfung auf Einhaltung der Elastizitätsgrenze (Fagus s. ca. 520 µm über 200~mm Länge des Elastos). \n",
    "Elasto(90) mit 429.6 mit der maximalen Faserdehnung"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ptq_df_elasto_min_max = ptq_df_elasto_summary.loc[['min', 'max']]\n",
    "ptq_df_elasto_min_max"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "latex_string = ptq_df_elasto_min_max.to_latex(index=True, escape=True, column_format=\"r|rrrr\", float_format=\"{:0.2f}\".format)\n",
    "\n",
    "caption = \"Feldversuch 2 - Ergebnisse, Maximale Dehnungswerte der Elastometer\"\n",
    "caption_long = \"Feldversuch 2 - Ergebnisse, Maximale Dehnungswerte der Elastometer in µm über die Länge der Messgeräte von 200~mm, jeweils der maximale gemessene Werte über alle Messungen, positive Werte = Faserdehnung, negative Werte = Faserstauchung\"\n",
    "save_latex_table(latex_string, caption, latex_export_directory, caption_long)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Für jedes Elasto innerhalb jeder Messung getrennt. Werte werden bei der Gesamtauswertung später verwendet."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Gruppiere nach ID und berechne min und max für jede Elasto-Spalte\n",
    "ptq_df_elasto_min_max = ptq_df.groupby('ID')[elasto_names].agg(['min', 'max'])\n",
    "ptq_df_elasto_min_max.head(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "latex_string = ptq_df_elasto_min_max.reset_index().to_latex(index=False, escape=True, column_format=\"r|rr|rr|rr|rr\", float_format=\"{:0.2f}\".format)\n",
    "\n",
    "caption = \"Feldversuch 2 - Ergebnisse, Maximalwerte Elastometer vollständig\"\n",
    "caption_long = \"Feldversuch 2 - Ergebnisse, Maximalwerte Elastometer vollständig, Faserstauchung als min., Faserdehnung als max. in µm über 200~mm Länge Messgerät\"\n",
    "save_latex_table(latex_string, caption, latex_export_directory, caption_long)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Die maximalen Dehnungs- und Stauchungswerte der Elastometer werden in der Struktur angepasst, um später mit den anderen Daten zusammenzuführen."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ptq_df_elasto = ptq_df_elasto_min_max.stack(level=0, future_stack=True).reset_index()\n",
    "\n",
    "ptq_df_elasto = ptq_df_elasto.rename(columns={\n",
    "    'ID': 'id', \n",
    "    'level_1': 'sensor_name',\n",
    "    'max': 'max_strain',\n",
    "    'min': 'max_compression'\n",
    "})\n",
    "\n",
    "ptq_df_elasto.head(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### PTQ: Plotten aller Messungen bzw. Elastometer\n",
    "\n",
    "Plotten der 4 verwendeten Elastometer in einem Plot für jede Messung. Die Plots werden im Verzeichnis ptq/plots/multi_sensors_vs_time_1/ abgelegt.\n",
    "Für alle Messungen und Elastometer ist gut zu erkennen, wie die Faserdehnung während des zusammen ziehen der Stämmlinge zunimmt, dann im Moment des Realises plötzlich abfällt, um in Folge harmonisch gedämpft auszuschwingen (nährungsweise)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ptq_series.plot_measurement_sensors(sensor_names=elasto_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### PTQ: Bestimmung von Schwingungsparametern\n",
    "\n",
    "#### PTQ: Selektieren der relevanten Bereiche\n",
    "\n",
    "Selektiere die Bereiche nach dem Release, bei dem es zu einer harmonisch gedämpften Schwingung kommt. Die Methode Series.get_oscillations sucht im Standardfall nach einem Bereich in den Messdaten mit einer Länge von 20 Sekunden. Der Anfangszeitpunkt wird durch einen plötzlichen Abfall der Dehnung auf unter Null bestimmt, bei dem die Steigung mindestens -25 beträgt. Die Suche nach dem Startzeitpunkt beginnt erst 60 Sekunden nach Messungsbeginn. Der entsprechende Code befindet sich im Paket classes/measurement.py und utils/select_oscillation.py. Die so isolierten Bereiche werden als Instanzen der Klasse Oscillation initialisiert. Weitere Parameter wie Amplitude, Frequenz und Dämpfung werden direkt berechnet.\n",
    "Parameter:\n",
    "- sensor_names: Eine Liste der Sensornamen, für die die Schwingungsdaten identifiziert werden sollen.\n",
    "- min_time_default: Die Mindestzeitspanne nach Beginn der Messung, nach der die Suche nach Schwingungen beginnt (Standard: 60 Sekunden).\n",
    "- min_value: Der minimale Wertschwellenwert, damit Sensordaten als gültig betrachtet werden.\n",
    "- threshold_slope: Der Steigungsschwellenwert, um den Beginn einer Schwingung zu bestimmen.\n",
    "- duration: Die Dauer, für die die Schwingungsdaten extrahiert werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ptq_series.get_oscillations(\n",
    "    sensor_names=elasto_names,\n",
    "    min_time_default=60,\n",
    "    min_value=50,\n",
    "    threshold_slope=-50,\n",
    "    duration=17.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### PTQ: Selektion optisch prüfen in Plots\n",
    "\n",
    "Plotten der relevanten Sensoren bzw. der selektierten Bereiche. Die Plots werden im Verzeichnis ptq/plots/select_oscillations_single/ bzw. ptq/plots/select_oscillations_combined/ gespeichert. In einem Combined-Plot werden alle 4 Elastometer einer Messung gemeinsam dargestellt. Hier wird manuell anhand der Plots geprüft, ob für alle Messungen und Sensoren der richtige Bereich ausgewählt wurde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ptq_series.plot_oscillations_for_measurements(sensor_names=elasto_names, combined=False)\n",
    "ptq_series.plot_oscillations_for_measurements(sensor_names=elasto_names, combined=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### PTQ: Anpassen der harmonisch gedämpften Schwingung\n",
    "\n",
    "Aus der PTQ-Messreihe 'ptq_series' wird über `get_oscillations_list` für alle Messungen, getrennt für jeden Sensor (Elastometer), die `Oscillation`-Instanz in eine Liste zusammengeführt. Entsprechend gibt es für jede PTQ-Messung 4 `Oscillation`-Instanzen (für die 4 Elastometer).\n",
    "\n",
    "`oscillation.fit` passt alle Schwingungsdaten mit einer allgemeinen Funktion für harmonisch gedämpfte Schwingungen an:\n",
    "\\[\n",
    "y(t) = A \\cdot e^{-\\delta t} \\cdot \\cos(\\omega_d \\cdot t + \\phi) + y_0\n",
    "\\]\n",
    "\n",
    "##### Parameterbeschreibung:\n",
    "- `A` (Anfangsamplitude): Der Anfangswert der Amplitude der Schwingung. Dieser Parameter bestimmt die initiale Höhe der Schwingungsamplitude.\n",
    "- `δ` (Dämpfungskoeffizient): Dieser Wert bestimmt, wie schnell die Amplitude der Schwingung mit der Zeit abnimmt. Ein höherer Wert führt zu einer schnelleren Dämpfung der Schwingung.\n",
    "- `ω_d` (gedämpfte Kreisfrequenz): Die Frequenz der gedämpften Schwingung in Radiant pro Sekunde. Dieser Parameter bestimmt, wie schnell die Schwingung oszilliert.\n",
    "- `φ` (Phasenwinkel): Der Anfangsphasenwinkel der Schwingung. Dieser Wert bestimmt den Startpunkt der Schwingung im Schwingungszyklus.\n",
    "- `y_0` (Vertikale Verschiebung): Dieser Parameter verschiebt die gesamte Schwingungskurve vertikal und ermöglicht es, die Schwingung an die mittlere Position der Daten anzupassen.\n",
    "- `t_0` (Horizontale Verschiebung): Dieser Parameter verschiebt die gesamte Schwingungskurve horizontal über die Zeit und ermöglicht es, die Schwingung an den spezifischen Startpunkt der gemessenen Schwingung anzupassen.\n",
    "\n",
    "(siehe `ptq/analyse/fitting_function.py`)\n",
    "\n",
    "### Zusätzliche Parameter und Konfigurationen:\n",
    "- **Startwerte und Grenzwerte:** Für die Optimierung der Parameter in `scipy.curve_fit` werden Startwerte und Grenzwerte für jeden Parameter übergeben (in `ptq/config.py` definiert).\n",
    "- **Qualitätsmetriken:** Zur Bewertung der Anpassungsgüte werden Metriken wie MAE (mittlerer absoluter Fehler), RMSE (Root Mean Square Error), und \\( R^2 \\) (Bestimmtheitsmaß) berechnet. Zusätzlich werden normalisierte Varianten (NRMSE und NMAE) zur besseren Vergleichbarkeit verwendet.\n",
    "- **Warnungen bei Überschreitung der Grenzwerte:** Wenn die für eine Metrik definierten Grenzwerte überschritten werden, wird eine Warnung im Log-Protokoll vermerkt, um auf mögliche Probleme bei der Anpassung hinzuweisen (in `ptq/config.py` definiert). Auf Basis dieser Warnung können:\n",
    "  - Start- und Grenzwerte sowie die Methodik angepasst werden.\n",
    "  - Betroffene Datensätze später ausgeschlossen werden, um fehlerhafte Anpassungen zu vermeiden.\n",
    "- **Interpolation:** Diese Option aktiviert die Interpolation der Datenpunkte, um eine ausreichende Dichte für `curve_fit` zu gewährleisten. Hierbei wird `scipy.interpolate.PchipInterpolator` verwendet, um Über- und Unterschwingungen, die nicht in den Originaldaten vorhanden sind, zu vermeiden. Nach optischer Prüfung zeigte diese Methode die besten Ergebnisse.\n",
    "\n",
    "##### Visualisierungsoptionen:\n",
    "- **Plot:** Wenn auf `True` gesetzt, wird für jede Oscillation ein Plot der angepassten Funktion zusammen mit den Originaldaten erstellt und in `working_dir/PTQ/plots/` gespeichert.\n",
    "- **Plot-Fehlerverteilung:** Wenn `plot_error` auf `True` gesetzt ist, wird ein Histogramm der Fehlerverteilung (Residuen) für jeden Fit erstellt und ebenfalls in `working_dir/PTQ/plots/` gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ptq_oscillations_ls = ptq_series.get_oscillations_list()\n",
    "\n",
    "initial_param = {\n",
    "    \"initial_amplitude\": 170,\n",
    "    \"damping_coeff\": 0.32,\n",
    "    \"frequency\": 0.44,\n",
    "    \"phase_angle\": 0,\n",
    "    \"y_shift\": 0,\n",
    "    \"x_shift\": 0\n",
    "}\n",
    "\n",
    "param_bounds = {\n",
    "    \"initial_amplitude\": (150, 250),\n",
    "    \"damping_coeff\": (0.1, 1),\n",
    "    \"frequency\": (0.35, 0.58),\n",
    "    \"phase_angle\": (-0.2, 0.2),\n",
    "    \"y_shift\": (-60, 60),\n",
    "    \"x_shift\": (-0.25, 0.75),\n",
    "}\n",
    "\n",
    "metrics_warning = {\n",
    "    \"pearson_r\": (0.75, 1),\n",
    "    \"nrmse\": (0, np.inf),\n",
    "    \"mae\": (0, np.inf),\n",
    "    \"nmae\": (0, 0.10)\n",
    "}\n",
    "\n",
    "for oscillation in ptq_oscillations_ls:\n",
    "    oscillation.fit(initial_param, param_bounds, optimize_criterion=\"mae\", metrics_warning=metrics_warning, plot=True,\n",
    "                    plot_error=True, dir_add=\"\", interpolate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Fehlerverteilung der Funktionsanpassung an Messdaten\n",
    "\n",
    "Die Funktion sammelt für alle Oscillation-Objekte die Fehler-Arrays der Anpassung und normalisiert die Fehler (um Unterschiede in der Skalierung zu entfernen).\n",
    "Anschließend werden die Fehler für alle Messungen A) für alle Sensoren gemeinsam und B) getrennt für jeden Sensor geplotet. Es werden Q-Q-Plot, Violin-Plot und Histogramme für den gleichen Sachverhalt erstellt und in `working_directory\\PTQ\\plots\\series_osc_errors` abgelegt.\n",
    "\n",
    "- `trim_hist_percent`: Beschneidet die Daten Links und Rechts um die äußersten x Prozent, da die Verteilung im Zentrum sonst kaum zu bewerten ist. Wirkt sich nur auf die Histogramme aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_normalized_errors = ptq_series.plot_osc_errors(plot_qq=True, plot_violin=True, plot_hist=True, hist_trim_percent=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T09:59:03.316646700Z",
     "start_time": "2023-12-01T09:59:01.399969500Z"
    },
    "collapsed": false
   },
   "source": [
    "#### PTQ: Zusammenfassung der Schwingungsparameter aller Oscillations als DataFrame\n",
    "\n",
    "'ptq_series.get_oscillations_df' fasst aus allen Oscillation-Instanzen der Messreihe ('ptq_series') die Schwingungsparameter als pandas.DataFrame zusammen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ptq_metadata_df = ptq_series.get_oscillations_df()\n",
    "# Converting 'sensor_name' to categorical\n",
    "ptq_metadata_df['sensor_name'] = ptq_metadata_df['sensor_name'].astype('category')\n",
    "\n",
    "metrics_warning_count = ptq_metadata_df['metrics_warning'].sum()\n",
    "print(f\"metrics_warning_count: {metrics_warning_count}\")\n",
    "ptq_metadata_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### PTQ: Berechnung ergänzender Schwingungsparameter\n",
    "\n",
    "Ergänze rechnerische Schwingungsparameter die Rust(2013) genutzt hat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_additional_parameters(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate additional parameters (ungedämpfte Frequenz, Dämpfungsgrad)\n",
    "    and append them to the DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): Input DataFrame with necessary columns, where 'frequency' is in Hz.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with additional parameters added.\n",
    "    \"\"\"\n",
    "    # Erstelle eine Kopie des DataFrames, um inplace-Operationen zu vermeiden\n",
    "    df = df.copy()\n",
    "\n",
    "    # Umbenennen der Spalte 'frequency' zu 'frequency_damped'\n",
    "    df = df.rename(columns={'frequency': 'frequency_damped'})\n",
    "\n",
    "    # Berechnung der ungedämpften Frequenz\n",
    "    df['frequency_undamped'] = np.sqrt(df['frequency_damped']**2 + (df['damping_coeff'] / (2 * np.pi))**2)\n",
    "\n",
    "    # Berechnung des Dämpfungsgrades\n",
    "    df['damping_ratio'] = df['damping_coeff'] / df['frequency_damped']\n",
    "\n",
    "    return df\n",
    "\n",
    "# Add additional parameters\n",
    "ptq_metadata_df = calculate_additional_parameters(ptq_metadata_df)\n",
    "ptq_metadata_df.head(5)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PTQ: Daten Vorbereitung (Data Cleaning)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ergänze maximale Dehnung und Stauchung zum ptq_metadata_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Merging ptq_metadata_df and ptq_df_elasto\n",
    "ptq_metadata_df = pd.merge(ptq_metadata_df, ptq_df_elasto, on=['id', 'sensor_name'], how='left')\n",
    "ptq_metadata_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Liste der Spalten, die beibehalten werden sollen\n",
    "select_cols = [\n",
    "    'id', \n",
    "    'sensor_name',\n",
    "    'max_strain',\n",
    "    'max_compression',\n",
    "    'm_amplitude', \n",
    "    'm_amplitude_2', \n",
    "    'initial_amplitude',\n",
    "    'damping_coeff', \n",
    "    'damping_ratio', \n",
    "    'frequency_damped', \n",
    "    'frequency_undamped', \n",
    "    'y_shift',\n",
    "    'pearson_r',\n",
    "    'nrmse', \n",
    "    'nmae', \n",
    "]\n",
    "\n",
    "# DataFrame auf die gewünschten Spalten reduzieren\n",
    "ptq_metadata_df = ptq_metadata_df[select_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ptq_metadata_df.head(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LS3: Daten Vorbereitung (Data Cleaning)\n",
    "Definition relevanter Spalten aus ls3_metadata_df und Anpassung der Datenstruktur, um nur die erforderlichen Variablen abzufragen. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Selecting relevant columns and creating a copy to avoid SettingWithCopyWarning\n",
    "select_cols = ['sensor_id', 'measurement_id', 'max', 'release']\n",
    "ls3_metadata_df = ls3_metadata_df[select_cols].copy()\n",
    "\n",
    "# Extracting values from tuples in 'max'\n",
    "ls3_metadata_df['max'] = ls3_metadata_df['max'].apply(lambda x: x[1] if isinstance(x, tuple) else x)\n",
    "ls3_metadata_df.head(3)\n",
    "ls3_metadata_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Combined: Zusammenführen der Daten von LS3, PTQ und Versuchsprotokoll (Merge)\n",
    "\n",
    "Führe Metadaten des Versuchsablaufes zusammen mit Daten der Elastometer (Schwingungsparameter und Metadaten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.merge(ptq_metadata_df, series_df, on='id', how='left')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ergänze die Daten der Sensorpositionierung"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Merging df and elasto_df\n",
    "# Geräte vom Typ Elastometer auswählen aus dem sensor_df\n",
    "elasto_df = sensor_df[sensor_df[\"type\"] == \"Elasto\"].copy()\n",
    "\n",
    "# Passe die sensor_namen einheitlich für beide DataFrames an\n",
    "elasto_df[\"sensor_name\"] = elasto_df[\"type\"].astype(str) + \"(\" + elasto_df[\"sensor_id\"].astype(str) + \")\"\n",
    "\n",
    "# Wähle nur die relevanten spalten aus\n",
    "select_cols = [\"sensor_name\", \"location\", \"height\", \"diameter\", \"direction\"]\n",
    "elasto_df = elasto_df[select_cols]\n",
    "\n",
    "# Perform the left join on sensor_name\n",
    "df = df.merge(elasto_df, on=\"sensor_name\", how=\"left\")\n",
    "\n",
    "# Convert sensor_name back to a categorical type\n",
    "df[\"sensor_name\"] = df[\"sensor_name\"].astype(\"category\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ergänze die Metadaten der Kraftmessdosen (LS3), hier zwei Kraftmessdosen mit verschiedener Aussage und jeweils für jede Messung aktiv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to prepare ls3_metadata\n",
    "def prepare_ls3_metadata(df, sensor_id, new_prefix):\n",
    "    return (df.query(f\"sensor_id == '{sensor_id}'\")\n",
    "            .drop('sensor_id', axis=1)\n",
    "            .add_prefix(new_prefix)\n",
    "            .rename(columns={f'{new_prefix}measurement_id': sensor_id}))\n",
    "\n",
    "# Merging ls3_metadata with the main DataFrame\n",
    "df = df.merge(prepare_ls3_metadata(ls3_metadata_df, '14:BF:E6', 'rope_'), on='14:BF:E6', how='left')     \n",
    "df = df.merge(prepare_ls3_metadata(ls3_metadata_df, '14:99:1E', 'cable_'), on='14:99:1E', how='left')     \n",
    "df.head(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Combined: Daten Vorbereitung (Data Cleaning)\n",
    "\n",
    "Filterung und Kategorisierung der Daten für weitergehende Analysen.\n",
    "\n",
    "### Combined: Filtern der relevanten Messungen aus Versuchsprotokoll\n",
    "\n",
    "Messung id = 1 war eine Vorabtest des Versuchsaufbaus und wurde mit einer unregelmäßigen Vorspannung von 2.5 durchgeführt.\n",
    "Messung id = 29 war der einzige Versuch mit der Behandlung cobra_static_slack.\n",
    "Die entsprechenden 2 Messungen werden entfernt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filtering and cleaning \n",
    "df = (df.drop(['14:BF:E6', '14:99:1E', 'cable_release', 'rope_max', 'time'], axis=1)\n",
    "      .query(\"release_force_target in [2.0, 2.4, 2.8] and treatment in ['free', 'gefa_dynamic', 'cobra_static']\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Nicht mehr genutzte Kategorien entfernen\n",
    "df[\"treatment\"] = df[\"treatment\"].cat.remove_unused_categories()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Datendokumentation:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Lade das Dictionary mit der Daten Dokumentation\n",
    "with open(data_path / \"feature_data_dict.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    feature_data_dict = json.load(f)\n",
    "\n",
    "data_dict = {**sensor_position_dict, **series_dict, **feature_data_dict}\n",
    "\n",
    "# Keys aller gewünschten Variablen (aus kombiniertem Dict)\n",
    "selected_keys = [\"id\", \"treatment\", \"sensor_name\", \"location\", \"height\", \"diameter\", \"rope_release\", \"max_strain\", \"calc_max_strain\", \"strain_difference\", \"release_force_target\", \"cable_max\", \"max_compression\", \"m_amplitude\", \"m_amplitude_2\", \"initial_amplitude\", \"damping_coeff\", \"damping_ratio\", \"frequency_damped\", \"frequency_undamped\", \"y_shift\", \"pearson_r\", \"nmae\"]\n",
    "\n",
    "# Nutze bestehende Felder\n",
    "selected_fields = [\"Zeichen\", \"Variable\", \"Deutsch\", \"Einheit\", \"Beschreibung\"]\n",
    "\n",
    "# Erzeuge DataFrame für LaTeX mit einheitlichem Aufbau\n",
    "combined_dict_df = extract_latex_dict_from_json(data_dict, selected_keys, selected_fields)\n",
    "\n",
    "# Exportiere als LaTeX\n",
    "latex_string = combined_dict_df.to_latex(index=False, escape=False)\n",
    "\n",
    "caption = \"Feldversuch 2 - Ergebnisse, Daten Dokumentation\"\n",
    "caption_long = \"Feldversuch 2 - Ergebnisse, Kräfte, Dehnungen und Schwingungsparameter, Daten Dokumentation\"\n",
    "\n",
    "save_latex_table(latex_string, caption, latex_export_directory, caption_long=caption_long)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ausgabe für CHAT-GPT import"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv(working_directory / 'export_data/2023_Kronensicherung_Plesse_Kraefte_Schwingungen_Full_Dataset.bz2', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Speichern\n",
    "data_dict_path = working_directory / 'export_data/2023_Kronensicherung_Plesse_Kraefte_Schwingungen_Full_Data_Dict.json'\n",
    "with open(data_dict_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data_dict, f, indent=2, ensure_ascii=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#df.dtypes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Combined: Analyse der zusammengeführten Daten für LS3, PTQ und Versuchsprotkoll"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Combined: Explorative Datenanalyse"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Combined: Definition von Darstellungsstandards\n",
    "Festlegen von Farbcodes für einheitliche Darstellung von Sensoren und Behandlungsvarianten für alle nachfolgenden Plots."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "PLOT_MANAGER = ptq_series.PLOT_MANAGER\n",
    "\n",
    "# Seaborn \"deep\" Palette holen\n",
    "deep_palette = sns.color_palette(\"deep\", 6)\n",
    "\n",
    "# Zuweisen von Farben aus der \"deep\" Palette an die Sensoren\n",
    "sensor_color_dict = {sensor: color for sensor, color in zip(elasto_names, deep_palette[:len(elasto_names)])}\n",
    "\n",
    "# Zuweisen von Farben aus der \"deep\" Palette an die Treatments\n",
    "treatment_color_dict = {treatment: color for treatment, color in zip(treatment_order, deep_palette[:len(treatment_order)])}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plotting Tite-, Achsen-, Filenamen erzeugen aus Data-Dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_axis_label(key: str, data_dict: Dict[str, Dict[str, str]]) -> str:\n",
    "    return f\"{data_dict[key]['Deutsch']} {data_dict[key]['Zeichen']} [{data_dict[key]['Einheit']}]\"\n",
    "\n",
    "def get_plot_title(x_key: str, y_key: str, data_dict: Dict[str, Dict[str, str]], prefix: str = \"Regression\") -> str:\n",
    "    return (\n",
    "        f\"{prefix}: {data_dict[y_key]['Deutsch']} \"\n",
    "        f\"({data_dict[y_key]['Zeichen']}) vs. \"\n",
    "        f\"{data_dict[x_key]['Deutsch']} \"\n",
    "        f\"({data_dict[x_key]['Zeichen']})\"\n",
    "    )\n",
    "\n",
    "def get_legend_title(key: str, data_dict: Dict[str, Dict[str, str]]) -> str:\n",
    "    return data_dict[key][\"Deutsch\"]\n",
    "\n",
    "def get_filename(x_key: str, y_key: str, prefix: str) -> str:\n",
    "    return slugify(f\"{prefix}_{y_key}_vs_{x_key}\", separator=\"_\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LS3: Analyse Spitzenlasten in der KS\n",
    "\n",
    "Analyse der Spitzenlasten in der KS gruppiert nach Ziel-Vorspannung und Treatment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Filtern und Umbenennen\n",
    "filtered_df = (\n",
    "    df[df['treatment'].isin(['gefa_dynamic', 'cobra_static'])]\n",
    "    .rename(columns={\n",
    "        'release_force_target': 'Ziel-Vorspannung',\n",
    "        'treatment': 'Behandlungsvariante',\n",
    "        'cable_max': 'Kraftspitze KS'\n",
    "    })[['Behandlungsvariante', 'Ziel-Vorspannung', 'Kraftspitze KS']]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Gruppieren, Aggregieren und Pivotieren\n",
    "pivoted_values = (\n",
    "    filtered_df\n",
    "    .groupby(['Ziel-Vorspannung', 'Behandlungsvariante'], observed=True)['Kraftspitze KS']\n",
    "    .agg(['min', 'mean', 'max'])\n",
    "    .unstack(level=0)\n",
    "    .swaplevel(axis=1)\n",
    "    .sort_index(axis=1, level=[0, 1], ascending=[True, False])\n",
    ")\n",
    "# Begrenze die Werte im Index Level 0 auf zwei Nachkommastellen\n",
    "pivoted_values.columns = pd.MultiIndex.from_tuples(\n",
    "    [\n",
    "        (f\"{level_0:.2f}\", level_1) if isinstance(level_0, float) else (level_0, level_1)\n",
    "        for level_0, level_1 in pivoted_values.columns\n",
    "    ],\n",
    "    names=pivoted_values.columns.names\n",
    ")\n",
    "\n",
    "# Ergebnis anzeigen\n",
    "pivoted_values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "latex_string = pivoted_values.to_latex(index=True, escape=True, float_format=\"{:0.2f}\".format, column_format=\"l|rrr|rrr|rrr\", multicolumn=True,\n",
    "    multicolumn_format=\"c\")\n",
    "\n",
    "caption = \"Feldversuch 2 - Ergebnisse, Spitzenlasten in der KS\"\n",
    "caption_long = \"Feldversuch 2 - Spitzenlasten in der KS gruppiert über Ziel-Vorspannung und Behandlungsvariante, angegeben ist jeweils pro Gruppe das Minimum, der Mittelwert und das Maximum, die Variante 'free' ist nicht aufgeführt, da hier keine KS eingesetzt wurde, alle Werte in kN\"\n",
    "\n",
    "save_latex_table(latex_string, caption, latex_export_directory, caption_long)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Filtern der Einträge mit dem maximalen Wert in 'cable_max'\n",
    "max_cable_max = df['cable_max'].max()\n",
    "filtered_df = df[df['cable_max'] == max_cable_max]\n",
    "\n",
    "# Innerhalb der gefilterten Einträge den maximalen 'max_strain' finden\n",
    "max_value_row = filtered_df.loc[filtered_df['max_strain'].idxmax()]\n",
    "#max_value_row"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Liste der gewünschten Spalten in der gewünschten Reihenfolge\n",
    "columns_to_display = [\n",
    "    'id', 'treatment', 'release_force_target', 'rope_release', \n",
    "    'cable_max', 'sensor_name', 'max_strain', 'max_compression'\n",
    "]\n",
    "\n",
    "# Zeile auf die gewünschten Spalten in der angegebenen Reihenfolge beschränken\n",
    "max_value_row_filtered = max_value_row[columns_to_display]\n",
    "\n",
    "max_value_row_filtered['treatment'] = slugify(max_value_row_filtered['treatment'])\n",
    "\n",
    "# Erstelle ein DataFrame mit den zusätzlichen Informationen aus `data_dict`\n",
    "expanded_data = []\n",
    "for col in columns_to_display:\n",
    "    expanded_data.append({\n",
    "        \"Zeichen\": data_dict[col][\"Zeichen\"],\n",
    "        \"Deutsch\": data_dict[col][\"Deutsch\"],\n",
    "        \"Wert\": max_value_row_filtered[col],\n",
    "        \"Einheit\": data_dict[col][\"Einheit\"],\n",
    "    })\n",
    "\n",
    "# Neues DataFrame erstellen\n",
    "expanded_df = pd.DataFrame(expanded_data)\n",
    "\n",
    "# LaTeX-String erstellen\n",
    "latex_string = expanded_df.to_latex(\n",
    "    index=False, \n",
    "    escape=False, \n",
    "    column_format=\"llrr\",  # Spaltenformat angepasst\n",
    "    float_format=\"{:0.2f}\".format\n",
    ")\n",
    "\n",
    "# LaTeX-Tabelle mit Beschriftung versehen\n",
    "caption = \"Feldversuch 2 - Ergebnisse, Werte der Messung mit Spitzenlast\"\n",
    "caption_long = \"Feldversuch 2 - Ergebnisse, Messung mit der höchsten Last in der KS (nur maximale Messwerte des Elastometers mit der höchsten Faserdehnung)\"\n",
    "\n",
    "# Funktion zum Speichern aufrufen (angenommen save_latex_table ist definiert)\n",
    "save_latex_table(latex_string, caption, latex_export_directory, caption_long)\n",
    "\n",
    "# Ergebnis anzeigen\n",
    "expanded_df\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Combined: Vergleichende Berechnung der Randfaserdehnung"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Funktion zur Berechnung der Randfaserdehnung (in Dezimalschreibweise)\n",
    "def calculate_epsilon(force: float, H: float, h: float, d: float, E: float) -> float:\n",
    "    \"\"\"\n",
    "    Berechnet die Randfaserdehnung epsilon (dimensionslos, nicht in %).\n",
    "    \n",
    "    Parameters:\n",
    "        force (float): Zugkraft im Seil (in kN)\n",
    "        H (float): Höhe des Angriffspunktes der Zugkraft über dem Stammfuß (in m)\n",
    "        h (float): Höhe des Berechnungspunktes über dem Boden (in m)\n",
    "        d (float): Durchmesser des Stammquerschnitts auf Höhe h (in m)\n",
    "        E (float): Elastizitätsmodul des Holzes (in MPa)\n",
    "    \n",
    "    Returns:\n",
    "        float: Berechnete Randfaserdehnung epsilon (dimensionslos)\n",
    "    \"\"\"\n",
    "    force_N = force * 1000  # Umrechnung der Zugkraft von kN in N\n",
    "    M = force_N * (H - h)  # Biegemoment in N·m\n",
    "    y_max = d / 2  # maximaler Abstand zur neutralen Achse in m\n",
    "    I = (np.pi / 64) * d**4  # Flächenträgheitsmoment in m^4\n",
    "    E_Pa = E * 1e6  # Umrechnung des Elastizitätsmoduls von MPa in N/m^2\n",
    "    epsilon = (M * y_max) / (E_Pa * I)  # Dehnung in Dezimalschreibweise\n",
    "    return epsilon\n",
    "\n",
    "# Funktion zur Berechnung der absoluten Längenänderung\n",
    "def calculate_delta_l(epsilon: float, l0: float) -> float:\n",
    "    \"\"\"\n",
    "    Berechnet die absolute Längenänderung Delta L in Mikrometer.\n",
    "    \n",
    "    Parameters:\n",
    "        epsilon (float): Relative Dehnung (dimensionslos)\n",
    "        l0 (float): Ausgangslänge des Elastometers (in mm)\n",
    "    \n",
    "    Returns:\n",
    "        float: Absolute Längenänderung Delta L in Mikrometer (µm)\n",
    "    \"\"\"\n",
    "    l0_m = l0 / 1000  # Umrechnung der Ausgangslänge von mm in m\n",
    "    delta_L = epsilon * l0_m * 1e6  # Umrechnung der Längenänderung in µm\n",
    "    return delta_L\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fixwerte\n",
    "l0 = 200  # mm, Ausgangslänge Elastometer\n",
    "E = 8500  # Elastizitätsmodul Buche in MPa\n",
    "height_rope = 18.30  # m, Höhe des Angriffspunktes der Zugkraft\n",
    "\n",
    "# Berechnung der neuen Spalten\n",
    "df['calc_max_strain_relativ'] = df.apply(lambda row: calculate_epsilon(\n",
    "    row['rope_release'], height_rope, row['height'], row['diameter'], E), axis=1)\n",
    "df['calc_max_strain'] = df['calc_max_strain_relativ'].apply(lambda epsilon: calculate_delta_l(epsilon, l0))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Berechnung der Differenz und der absoluten Differenz in Prozent\n",
    "df['strain_difference'] = (df['calc_max_strain'] - df['max_strain']) / df['max_strain'] * 100"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head(4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Grouping DataFrame by 'ptq_sensor_name'\n",
    "grouped = df.groupby('sensor_name')\n",
    "\n",
    "# Perform linear regression and plot for each group\n",
    "for name, group in grouped:\n",
    "    # Linear Regression with statsmodels\n",
    "    X = sm.add_constant(group['max_strain'])  # Adding a constant for the intercept\n",
    "    y = group['calc_max_strain']\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    # Print the summary of the linear regression model\n",
    "    print(f\"Linear Regression Summary for {name}:\\n\")\n",
    "    print(model.summary())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Achsenvariablen definieren\n",
    "x_key = 'max_strain'\n",
    "y_key = 'calc_max_strain'\n",
    "\n",
    "# Dynamische Labels und Titel erzeugen\n",
    "x_label = get_axis_label(x_key, data_dict)\n",
    "y_label = get_axis_label(y_key, data_dict)\n",
    "plot_title = get_plot_title(x_key, y_key, data_dict, prefix=\"Regression\")\n",
    "filename = get_filename(x_key, y_key, prefix=\"regression\")\n",
    "legend_title = get_legend_title(\"sensor_name\", data_dict)\n",
    "\n",
    "# Plot erstellen\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "for name, group in df.groupby('sensor_name', observed=False):\n",
    "    sns.regplot(\n",
    "        x=group[x_key],\n",
    "        y=group[y_key],\n",
    "        color=sensor_color_dict.get(name, \"gray\"),\n",
    "        label=name,\n",
    "        scatter_kws={\"s\": 40}\n",
    "    )\n",
    "\n",
    "plt.title(plot_title)\n",
    "plt.xlabel(x_label)\n",
    "plt.ylabel(y_label)\n",
    "plt.legend(title=legend_title)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot speichern\n",
    "PLOT_MANAGER.save_plot(fig, filename=filename, subdir=\"measured_vs_calc_strain\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Berechnung von Mittelwerten und Standardabweichungen\n",
    "df_strain_stats = df.groupby('sensor_name').agg(\n",
    "    calc_max_strain_mean=('calc_max_strain', 'mean'),\n",
    "    calc_max_strain_sd=('calc_max_strain', 'std'),\n",
    "    ptq_max_strain_mean=('max_strain', 'mean'),\n",
    "    ptq_max_strain_sd=('max_strain', 'std'),\n",
    "    strain_difference_mean=('strain_difference', 'mean'),\n",
    "    strain_difference_sd=('strain_difference', 'std')\n",
    ")\n",
    "# Automatische Umbenennung der Spalten basierend auf data_dict\n",
    "columns_new = [(data_dict[var]['Zeichen'], stat) for var, stat in zip(\n",
    "    ['calc_max_strain', 'calc_max_strain', 'max_strain', 'max_strain', 'strain_difference', 'strain_difference'],\n",
    "    ['mean', 'sd', 'mean', 'sd', 'mean', 'sd']\n",
    ")]\n",
    "df_strain_stats.columns = pd.MultiIndex.from_tuples(columns_new)\n",
    "df_strain_stats"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Berechnung des MAPE\n",
    "strain_difference_mape = df['strain_difference'].abs().mean().round(2)\n",
    "strain_difference_mape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Automatische Einheitenzuweisung\n",
    "df_units = pd.DataFrame([[data_dict[var]['Einheit'] for var in ['calc_max_strain', 'calc_max_strain', 'max_strain', 'max_strain', 'strain_difference', 'strain_difference']]],\n",
    "                        columns=df_strain_stats.columns, index=['Einheit'])\n",
    "\n",
    "# MAPE als eigene Zeile hinzufügen\n",
    "df_mape = pd.DataFrame([['', '', '', '', strain_difference_mape, '']], \n",
    "                        columns=df_strain_stats.columns, index=['MAPE'])\n",
    "\n",
    "# DataFrames zusammenführen\n",
    "df_strain_stats_add = pd.concat([df_units, df_strain_stats, df_mape])\n",
    "df_strain_stats_add"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Konvertierung des DataFrames mit Multi-Index-Spalten in einen LaTeX-String\n",
    "latex_string = df_strain_stats_add.to_latex(\n",
    "    index=True,\n",
    "    escape=False,\n",
    "    float_format=\"{:0.2f}\".format,\n",
    "    multicolumn=True,\n",
    "    multicolumn_format=\"c\",\n",
    "    column_format=\"l|rr|rr|rr\"\n",
    ")\n",
    "\n",
    "# Definition der Beschriftung für die LaTeX-Tabelle\n",
    "caption = \"Feldversuch 2 - Ergebnisse, Zusammenfassung Vergleich gemessene und rechnerische Faserdehnung \"\n",
    "caption_long = \"Feldversuch 2 - Ergebnisse, Zusammenfassung Vergleich gemessene und rechnerische Faserdehnung, Mittelwerte und Standardabweichungen der Abweichung der rechnerischen von der gemessenen maximalen Dehnung, gruppiert über Elastometer bzw. Position\"\n",
    "\n",
    "save_latex_table(latex_string, caption, latex_export_directory, caption_long)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create violin plot for strain_difference grouped by sensor_name\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(x='sensor_name', y='strain_difference', hue='sensor_name', palette=sensor_color_dict, data=df)\n",
    "plt.title('Difference between measured and calculated strain')\n",
    "plt.xlabel('PTQ Sensor Name / Position')\n",
    "plt.ylabel('Strain Difference (%)')\n",
    "# Layout anpassen und Plot anzeigen\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "PLOT_MANAGER.save_plot(fig, filename=\"diff_measured_vs_calc_strain\", subdir=\"measured_vs_calc_strain\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Theoretische Berechnungen der Belastung der KS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_force(epsilon: float, H: float, h: float, d: float, E: float) -> float:\n",
    "    \"\"\"\n",
    "    Berechnet die Zugkraft force (in kN) basierend auf der Randfaserdehnung epsilon bzw. der Dehnung an der Elastizitätsgrenze.\n",
    "    \n",
    "    Parameters:\n",
    "        epsilon (float): Randfaserdehnung (dimensionslos, nicht in %)\n",
    "        H (float): Höhe des Angriffspunktes der Zugkraft über dem Stammfuß (m)\n",
    "        h (float): Höhe des Berechnungspunktes über dem Boden (m)\n",
    "        d (float): Durchmesser des Stammquerschnitts auf Höhe h (m)\n",
    "        E (float): Elastizitätsmodul des Holzes (in MPa)\n",
    "    \n",
    "    Returns:\n",
    "        float: Berechnete Zugkraft force (in kN)\n",
    "    \"\"\"\n",
    "    y_max = d / 2  # maximaler Abstand zur neutralen Achse in m\n",
    "    I = (np.pi / 64) * d**4  # Flächenträgheitsmoment in m^4\n",
    "    E_Pa = E * 1e6  # Umrechnung des Elastizitätsmoduls von MPa in N/m^2\n",
    "    force_N = (epsilon * E_Pa * I) / ((H - h) * y_max)  # Zugkraft in N\n",
    "    force_kN = force_N / 1000  # Umrechnung in kN\n",
    "    return force_kN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fixwerte\n",
    "epsilon = 0.0026  # Elastizitätsgrenze Buche\n",
    "E = 8500  # Elastizitätsmodul Buche in MPa\n",
    "height_ks = 17.40  # m, Höhe des Angriffspunktes der KS\n",
    "height_forke = 10.31  # m, Höhe des Zwiesels\n",
    "stem_diameter = 0.35  # m, Durchmesser des Stammes knapp über dem Zwiesel\n",
    "\n",
    "# Berechnung der Zugkraft\n",
    "force = calculate_force(epsilon, height_ks, height_forke, stem_diameter, E)\n",
    "force"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tree_height = (26.15 + 27.2)/2\n",
    "optimal_ks_height = ((tree_height - height_forke) * (2/3)) + height_forke\n",
    "optimal_ks_height"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "force = calculate_force(epsilon, optimal_ks_height, height_forke, stem_diameter, E)\n",
    "force"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PTQ: Analyse der Dehnungswerte nach Elastometer und Behandlungsvarianten"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plotten der Maximalwerte der Vorspannung für Dehnung und Stauchung. Für jede Messung werden erst die maximalen Werte berechnet und dann getrennt nach Elasto die Verteilung im Boxplot dargestellt.\n",
    "Auffällig ist Elasto(90) mit den höchsten Dehnungswerten. Dieses Gerät ist am geringfügig dünneren Stämmling angebracht. Die Vorspannung wurde aufgrund der maximalen Messwerte von 429 µm nicht weiter erhöht."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Erstelle die Subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# Boxplot für die minimalen Werte (maximale Compression)\n",
    "sns.boxplot(data=df, x='sensor_name', y='max_compression', hue='sensor_name', palette=sensor_color_dict, ax=axs[0], legend=False)\n",
    "axs[0].set_title(\"Maximum Compression per Elasto\")\n",
    "axs[0].set_xlabel(\"Elasto Name\")\n",
    "axs[0].set_ylabel(\"fiber compression [µm]\")\n",
    "\n",
    "# Boxplot für die maximalen Werte (maximale Strain)\n",
    "sns.boxplot(data=df, x='sensor_name', y='max_strain', hue='sensor_name', palette=sensor_color_dict, ax=axs[1], legend=False)\n",
    "axs[1].set_title(\"Maximum Strain per Elasto\")\n",
    "axs[1].set_xlabel(\"Elasto Name\")\n",
    "axs[1].set_ylabel(\"fiber strain [µm]\")\n",
    "\n",
    "# Layout anpassen und Plot anzeigen\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "PLOT_MANAGER.save_plot(fig, filename=\"strain_max_per_elasto\", subdir=\"strain_max_per_elasto\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Mixed-Effects Model\n",
    "# model = smf.mixedlm(\n",
    "#     \"max_compression ~ C(treatment) + C(sensor_name) + rope_release\",\n",
    "#     data=df,\n",
    "#     groups=df[\"id\"]  # Gruppierung nach Beobachtungsgruppe\n",
    "# ).fit()\n",
    "# \n",
    "# # Ergebnisse\n",
    "# print(model.summary())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Tukey HSD Test für die 'treatment' Gruppe\n",
    "# tukey_treatment = mc.pairwise_tukeyhsd(endog=df['max_compression'],\n",
    "#                                     groups=df['treatment'],\n",
    "#                                     alpha=0.05)\n",
    "# print(tukey_treatment)\n",
    "# \n",
    "# # Tukey HSD Test für die 'sensor_name' Gruppe\n",
    "# tukey_sensor = mc.pairwise_tukeyhsd(endog=df['max_compression'],\n",
    "#                                  groups=df['sensor_name'],\n",
    "#                                  alpha=0.05)\n",
    "# print(tukey_sensor)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Combined: Latex-Export von Daten für Anhang"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_grouped_latex_tables(df_latex: pd.DataFrame, caption: str, column_format: str, group_by: str, latex_export_directory: Path) -> None:\n",
    "    \"\"\"\n",
    "    Generate grouped LaTeX tables for each unique value in a specified column of a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df_latex (pd.DataFrame): The DataFrame with already formatted columns.\n",
    "        caption (str): The caption for the LaTeX tables.\n",
    "        column_format (str): The format for the LaTeX tables.\n",
    "        group_by (str): The column name to group by.\n",
    "        latex_export_directory (Path): The directory to save the LaTeX tables.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # DataFrame grouped by specified column\n",
    "        grouped = df_latex.groupby(group_by, observed=True)\n",
    "\n",
    "        # LaTeX tables to be combined in a single file\n",
    "        combined_tables = []\n",
    "\n",
    "        for group, group_df in grouped:\n",
    "            # Drop the group_by column\n",
    "            group_df = group_df.drop(columns=[group_by])\n",
    "\n",
    "            # Format 'id' column as string if it exists\n",
    "            if 'id' in group_df.columns:\n",
    "                group_df['id'] = group_df['id'].astype(str)\n",
    "\n",
    "            # Calculate statistics\n",
    "            mean_row = group_df.mean(numeric_only=True).rename('Mean')\n",
    "            median_row = group_df.median(numeric_only=True).rename('Median')\n",
    "            sd_row = group_df.std(numeric_only=True).rename('SD')\n",
    "\n",
    "            # Combine stats with original DataFrame\n",
    "            stats_df = pd.concat([group_df, mean_row.to_frame().T, median_row.to_frame().T, sd_row.to_frame().T])\n",
    "\n",
    "            # Generate LaTeX string from DataFrame\n",
    "            df_latex_string = stats_df.to_latex(\n",
    "                index=True,\n",
    "                escape=False,\n",
    "                column_format=column_format,\n",
    "                float_format=\"{:0.2f}\".format\n",
    "            )\n",
    "\n",
    "            # Create caption and label for the group\n",
    "            caption_text = create_caption(caption, f\"{caption} für {slugify(group, separator=' ')}\")\n",
    "            label_clean = create_label(caption=caption, additional_label=group)\n",
    "\n",
    "            # Generate LaTeX table\n",
    "            latex_table = generate_latex_table(df_latex_string, caption_text, label_clean)\n",
    "\n",
    "            combined_tables.append(latex_table)\n",
    "\n",
    "        # Combine all tables and save to a single file\n",
    "        if combined_tables:\n",
    "            final_output = \"\\n\\n\".join(combined_tables)\n",
    "            file_name = create_label(caption) + \".tex\"\n",
    "            save_to_file(final_output, latex_export_directory / file_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating grouped LaTeX tables: {e}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Liste der benötigten Variablen\n",
    "variables = [\"id\", \"treatment\", \"sensor_name\", \"location\", \"height\", \"diameter\", \"rope_release\", \"max_strain\", \"calc_max_strain\", \"strain_difference\"]\n",
    "\n",
    "# DataFrame kopieren und die gewünschten Spalten auswählen\n",
    "df_latex = df.copy()[variables]\n",
    "\n",
    "# Spaltennamen mit den Kurzbezeichnungen (Zeichen) aus dem data_dict umbenennen\n",
    "df_latex = df_latex.rename(columns={var: data_dict[var][\"Zeichen\"] for var in variables})\n",
    "# Funktionsaufruf mit Beispielparametern\n",
    "generate_grouped_latex_tables(\n",
    "    df_latex=df_latex,\n",
    "    caption=\"Feldversuch 2 - Ergebnisse, Vergleich gemessene und rechnerische Faserdehnung\",\n",
    "    column_format=\"lll|lrrr|rrr\",\n",
    "    group_by=\"treatment\",\n",
    "    latex_export_directory=latex_export_directory\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "variables = ['id', 'sensor_name', 'treatment', 'release_force_target', 'rope_release', 'cable_max', 'max_strain', 'max_compression',]\n",
    "\n",
    "# DataFrame kopieren und die gewünschten Spalten auswählen\n",
    "df_latex = df.copy()[variables]\n",
    "\n",
    "# Spaltennamen mit den Kurzbezeichnungen (Zeichen) aus dem data_dict umbenennen\n",
    "df_latex = df_latex.rename(columns={var: data_dict[var][\"Zeichen\"] for var in variables})\n",
    "# Funktionsaufruf mit Beispielparametern\n",
    "generate_grouped_latex_tables(\n",
    "    df_latex=df_latex,\n",
    "    caption=\"Feldversuch 2 - Ergebnisse, Kräfte und Dehnungen\",\n",
    "    column_format=\"lrl|rrr|rr\",\n",
    "        group_by=\"treatment\",\n",
    "    latex_export_directory=latex_export_directory\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "variables_2 = ['id', 'sensor_name', 'treatment', 'm_amplitude', 'm_amplitude_2', 'initial_amplitude', 'damping_coeff', 'damping_ratio', 'frequency_damped', 'frequency_undamped', 'y_shift', 'pearson_r', 'nmae']\n",
    "\n",
    "# DataFrame kopieren und die gewünschten Spalten auswählen\n",
    "df_latex = df.copy()[variables_2]\n",
    "\n",
    "# Spaltennamen mit den Kurzbezeichnungen (Zeichen) aus dem data_dict umbenennen\n",
    "df_latex = df_latex.rename(columns={var: data_dict[var][\"Zeichen\"] for var in variables_2})\n",
    "# Funktionsaufruf mit Beispielparametern\n",
    "generate_grouped_latex_tables(\n",
    "    df_latex=df_latex,\n",
    "    caption=\"Feldversuch 2 - Ergebnisse, Schwingungsparameter\",\n",
    "    column_format=\"lrl|rrr|rr|rr|r|rr\",\n",
    "    group_by=\"treatment\",\n",
    "    latex_export_directory=latex_export_directory\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PTQ: Analyse der Schwingungsparameter\n",
    "\n",
    "In diesem Abschnitt werden die Schwingungsparameter statistisch ausgewertet. Ziel ist es, den Einfluss verschiedener Behandlungsvarianten (treatment) auf die gemessenen Schwingungsparameter zu untersuchen und dabei auch den potenziellen Einfluss der Vorspannung (rope_release) und Sensorposition (sensor_name) zu berücksichtigen.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "variables = [\n",
    "    'm_amplitude', \n",
    "    'm_amplitude_2',\n",
    "    'max_strain',\n",
    "    'max_compression',\n",
    "    'initial_amplitude',\n",
    "    'damping_coeff', \n",
    "    'damping_ratio', \n",
    "    'frequency_damped', \n",
    "    'frequency_undamped',\n",
    "    'pearson_r',\n",
    "    #'nrmse', \n",
    "    'nmae'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Systematischer Einfluss der Sensorposition\n",
    "\n",
    "Ziel: Visuell erkennen, ob unterschiedliche Sensoren konsistent andere Werte liefern und ob dieser Effekt die Interpretation der treatment-Effekte erschwert."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot erstellen\n",
    "for var in variables:\n",
    "    fig = plt.figure(figsize=(8, 5))\n",
    "    sns.boxplot(x=\"sensor_name\", y=var, data=df, hue=\"treatment\", palette=treatment_color_dict,  dodge=True)\n",
    "    # Stripplot: Punkte zur Veranschaulichung der Verteilung\n",
    "    sns.stripplot(x=\"sensor_name\", y=var, data=df, hue=\"treatment\", palette=treatment_color_dict, dodge=True, alpha=1, jitter=True, size=5, legend=False)\n",
    "    # Titel und Achsentitel setzen\n",
    "    plt.title(f\"Einfluss von treatment auf {var} gruppiert über sensor_name\")\n",
    "    plt.xlabel(\"Sensor Name\")\n",
    "    plt.ylabel(var)\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    PLOT_MANAGER.save_plot(fig, filename=f\"effect_sensor_treatment_{var}\", subdir=\"combined/sensor\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Systematischer Einfluss der Behandlungsvariante\n",
    "\n",
    "Ziel: Feststellen, ob die Variation durch unterschiedliche Behandlungen relativ zur sensorbedingten Variation unterscheidbar ist."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot erstellen\n",
    "for var in variables:\n",
    "    fig = plt.figure(figsize=(8, 5))\n",
    "    sns.boxplot(x=\"treatment\", y=var, data=df, palette=sensor_color_dict, hue='sensor_name')\n",
    "    sns.stripplot(x=\"treatment\", y=var, data=df, palette=sensor_color_dict, hue='sensor_name', dodge=True, alpha=1, jitter=True, size=5, legend=False)\n",
    "    plt.title(f\"Einfluss von sensor_name auf {var} gruppiert über treatment\")\n",
    "    plt.xlabel(\"treatment\")\n",
    "    plt.ylabel(var)\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    PLOT_MANAGER.save_plot(fig, filename=f\"effect_treatment_sensor_{var}\", subdir=\"combined/treatment\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Versuche Mixed-Linear Model\n",
    "Die Daten sind hierarchisch: Mehrere Messungen (vier Sensoren) pro Beobachtungseinheit (`id`). Ein Mixed-Effects Modell könnte diese Struktur abbilden, indem zufällige Effekte für `id` und feste Effekte für `treatment` sowie `sensor_name` genutzt werden. Zusätzlich könnte `rope_release` als Kovariate eingeführt werden.\n",
    "\n",
    "Diese Modelle wären theoretisch präziser, aber aufgrund der geringen Stichprobengröße und der komplexen Datenstruktur treten Konvergenzprobleme auf.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Ergebnisse als Dictionary speichern\n",
    "results_dict = {}\n",
    "\n",
    "for var in variables:\n",
    "    # Formel für MixedLM: Parameter ~ C(treatment) + C(sensor_name) + rope_release + (1|id)\n",
    "    formula = f\"{var} ~ C(treatment) + C(sensor_name)\" #  +  rope_release\n",
    "    model = smf.mixedlm(formula, data=df, groups=df[\"id\"])\n",
    "    \n",
    "    # Modell fitten\n",
    "    fit = model.fit(reml=True)  # REML ist Standard für gemischte Modelle\n",
    "    \n",
    "    # Ergebnisse ausgeben\n",
    "    print(f\"\\n### Ergebnisse für {var} ###\")\n",
    "    print(fit.summary())\n",
    "    \n",
    "    # Überprüfen, ob das Modell konvergiert ist\n",
    "    if not fit.converged:\n",
    "        print(\"Achtung: Das Modell ist nicht konvergiert. Erwägen Sie Anpassungen (z.B. Skalierung der Daten).\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Vereinfachtes Vorgehen durch Aggregation (Mittelung über Sensoren)\n",
    "\n",
    "Um dennoch aussagekräftige Aussagen zu erhalten, werden die Messungen pro `id` über alle Sensoren gemittelt. Dadurch geht zwar die Variation aufgrund unterschiedlicher Sensoren verloren, aber es entsteht ein stabileres Datenset, in dem jede `id` einen aggregierten Wert pro Parameter hat.\n",
    "\n",
    "Auf dieser Basis können einfache OLS-Modelle geschätzt werden, z. B. `Parameter ~ C(treatment) + rope_release`. Diese Modelle sind einfacher und sollten stabil konvergieren."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Placeholder for results\n",
    "results = {}\n",
    "\n",
    "# Step 1: Compute the mean of each variable for each id\n",
    "grouped_df = df.groupby(\"id\").agg(\n",
    "    {var: \"mean\" for var in variables}\n",
    ").reset_index()\n",
    "\n",
    "# Add additional variables (e.g., treatment and rope_release) to the aggregated data\n",
    "grouped_df = grouped_df.merge(\n",
    "    df[[\"id\", \"treatment\", \"rope_release\"]].drop_duplicates(), on=\"id\"\n",
    ")\n",
    "\n",
    "# Step 2: Fit linear models to the aggregated data\n",
    "for var in variables:\n",
    "    try:\n",
    "        # Fit a linear model\n",
    "        model = smf.ols(f\"{var} ~ C(treatment) + rope_release\", grouped_df).fit()\n",
    "\n",
    "        # Store relevant results\n",
    "        results[var] = {\n",
    "            \"p_value_treatment\": model.pvalues.get(\"treatment[T.statisch]\", None),\n",
    "            \"p_value_rope_release\": model.pvalues.get(\"rope_release\", None),\n",
    "            \"summary\": model.summary()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        results[var] = {\"error\": str(e)}\n",
    "\n",
    "# Display results\n",
    "results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nicht jeder Parameter wird durch `rope_release` beeinflusst. Nur für jene Parameter, bei denen ein signifikanter Einfluss von `rope_release` festgestellt wird, soll dieser Effekt herausgerechnet werden. Auf diese Weise entstehen \"bereinigte\" Werte, in denen der lineare Einfluss von `rope_release` entfernt ist."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Placeholder for results\n",
    "# Dieses Dictionary wird nun sowohl die Modellobjekte als auch die Kennwerte speichern.\n",
    "results = {}\n",
    "\n",
    "# Liste der Variablen, für die rope_release berücksichtigt wird\n",
    "relevant_vars_rope_release = ['m_amplitude', 'm_amplitude_2', 'max_strain', 'max_compression']\n",
    "\n",
    "# Step 1: Compute the mean of each variable for each id\n",
    "grouped_df = df.groupby(\"id\").agg(\n",
    "    {var: \"mean\" for var in variables}\n",
    ").reset_index()\n",
    "\n",
    "# Add additional variables (e.g., treatment and rope_release) to the aggregated data\n",
    "grouped_df = grouped_df.merge(\n",
    "    df[[\"id\", \"treatment\", \"rope_release\"]].drop_duplicates(), on=\"id\"\n",
    ")\n",
    "\n",
    "# Step 2: Fit linear models to the aggregated data and store model objects and p-Werte in results\n",
    "for var in variables:\n",
    "    # Formuliere das Modell dynamisch, abhängig davon, ob rope_release relevant ist\n",
    "    if var in relevant_vars_rope_release:\n",
    "        formula = f\"{var} ~ C(treatment) + rope_release\"\n",
    "    else:\n",
    "        formula = f\"{var} ~ C(treatment)\"\n",
    "\n",
    "    try:\n",
    "        # Fit a linear model\n",
    "        model = smf.ols(formula, grouped_df).fit()\n",
    "\n",
    "        # Store model and relevant results directly in results\n",
    "        results[var] = {\n",
    "            \"model\": model,\n",
    "            \"p_value_treatment\": model.pvalues.get(\"treatment[T.statisch]\", None),\n",
    "            \"p_value_rope_release\": model.pvalues.get(\"rope_release\", None) if var in relevant_vars_rope_release else None,\n",
    "            \"summary\": model.summary()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        results[var] = {\"error\": str(e)}\n",
    "\n",
    "# Display results\n",
    "print(results)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Beispiel für m_amplitude\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lmplot(data=grouped_df, x='rope_release', y='m_amplitude_2', hue='treatment', ci=95)\n",
    "plt.title('Einfluss von rope_release auf m_amplitude für verschiedene Treatments')\n",
    "plt.xlabel('rope_release (kN)')\n",
    "plt.ylabel('m_amplitude')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=grouped_df, x='treatment', y='m_amplitude')\n",
    "plt.title('Vergleich von m_amplitude zwischen den Treatments')\n",
    "plt.xlabel('Treatment')\n",
    "plt.ylabel('m_amplitude')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import re\n",
    "\n",
    "# Liste der Variablen, die in die Tabelle übernommen werden sollen\n",
    "variables_spec = [\n",
    "    'm_amplitude_2',\n",
    "    'max_compression',\n",
    "    'damping_coeff',\n",
    "    'damping_ratio',\n",
    "    'frequency_damped',\n",
    "    'frequency_undamped',\n",
    "]\n",
    "\n",
    "# Umbenennung der Spalten für LaTeX-Notation\n",
    "column_rename_map = {\n",
    "    'm_amplitude_2': r'$mA_2$',\n",
    "    'max_compression': r'$\\text{max\\_C}$',\n",
    "    'damping_coeff': r'$\\delta$',\n",
    "    'damping_ratio': r'$D$',\n",
    "    'frequency_damped': r'$f_d$',\n",
    "    'frequency_undamped': r'$f_0$',\n",
    "}\n",
    "\n",
    "# Funktion zur Erstellung der Modellgüte-Kennzahlen für alle Variablen\n",
    "def create_model_metrics_table(variables, results):\n",
    "    metrics_data = {\n",
    "        \"Kennzahl\": [\"R²\", \"Adj. R²\", \"F-St.\", \"AIC\", \"N\"]\n",
    "    }\n",
    "\n",
    "    for var in variables:\n",
    "        col_name = column_rename_map.get(var, var)  # Verwende gekürzten Namen, falls vorhanden\n",
    "        if var not in results or 'error' in results[var]:\n",
    "            metrics_data[col_name] = [\"n/a\"] * 5\n",
    "        else:\n",
    "            model = results[var]['model']\n",
    "            metrics_data[col_name] = [\n",
    "                f\"{model.rsquared:.4f}\",\n",
    "                f\"{model.rsquared_adj:.4f}\",\n",
    "                f\"{model.fvalue:.4f}\",\n",
    "                f\"{model.aic:.4f}\",\n",
    "                f\"{model.nobs:.0f}\"\n",
    "            ]\n",
    "\n",
    "    # Erstelle die Tabelle mit tabulate\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    return tabulate(\n",
    "        metrics_df,\n",
    "        headers=\"keys\",\n",
    "        tablefmt=\"latex_raw\",\n",
    "        floatfmt=\".4f\",\n",
    "        showindex=False)\n",
    "\n",
    "# Funktion zur Erstellung einer LaTeX-Tabelle für die Koeffizienten einer Variable\n",
    "def create_latex_table_for_variable(var, results):\n",
    "    if var not in results:\n",
    "        return f\"%% Keine Ergebnisse für Variable {var} vorhanden.\"\n",
    "    \n",
    "    model_result = results[var]\n",
    "    if 'error' in model_result:\n",
    "        return f\"%% Fehler beim Anpassen des Modells für {var}: {model_result['error']}\"\n",
    "    \n",
    "    model = model_result['model']\n",
    "    summary = model.summary2().tables[1]  # Zugriff auf die Tabelle der Koeffizienten\n",
    "\n",
    "    # Erstelle eine LaTeX-Tabelle mit tabulate für die Koeffizienten\n",
    "    latex_table = tabulate(\n",
    "        summary,\n",
    "        headers=summary.columns,\n",
    "        tablefmt=\"latex_booktabs\",\n",
    "        floatfmt=\".4f\",\n",
    "        showindex=True\n",
    "    )\n",
    "\n",
    "    # Escape Unterstriche in der Variable für LaTeX\n",
    "    escaped_var = re.sub(r'_', r'\\_', var)\n",
    "    shortened_var = column_rename_map.get(var, escaped_var)\n",
    "\n",
    "    # Füge die LaTeX-Caption zur Tabelle hinzu\n",
    "    return f\"\"\"\n",
    "\\\\begin{{table}}[ht]\n",
    "    \\\\centering\n",
    "    \\\\caption{{Modellzusammenfassung für {escaped_var} ({shortened_var})}}\n",
    "    \\\\begin{{adjustbox}}{{max width=\\\\textwidth}}\n",
    "    {latex_table}\n",
    "    \\\\end{{adjustbox}}\n",
    "\\\\end{{table}}\n",
    "\"\"\"\n",
    "\n",
    "# Erstelle die Modellgüte-Tabelle für alle Variablen\n",
    "latex_metrics_table = create_model_metrics_table(variables_spec, results)\n",
    "print(\"\"\"\n",
    "\\\\begin{table}[ht]\n",
    "    \\\\centering\n",
    "    \\\\caption{Modellgüte für alle Variablen}\n",
    "    \\\\begin{adjustbox}{max width=\\\\textwidth}\n",
    "\"\"\")\n",
    "print(latex_metrics_table)\n",
    "print(\"\"\"\n",
    "    \\\\end{adjustbox}\n",
    "\\\\end{table}\n",
    "\n",
    "\\\\vspace{1cm}\n",
    "\"\"\")\n",
    "\n",
    "# Erstelle und print die LaTeX-Tabellen für die Koeffizienten jeder Variable\n",
    "for var in variables_spec:\n",
    "    latex_output = create_latex_table_for_variable(var, results)\n",
    "    print(latex_output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Anzahl der relevanten Variablen und Layout für die Subplots definieren\n",
    "n_vars, n_cols, n_rows = len(variables), 2, (len(variables) + 1) // 2\n",
    "\n",
    "# Subplots erstellen\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(10, 4 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Für jede relevante Variable einen Plot erstellen\n",
    "for i, var in enumerate(variables):\n",
    "    sns.boxplot(ax=axes[i], x=\"treatment\", y=var, data=grouped_df, palette=treatment_color_dict, hue=\"treatment\", legend=False, dodge=False)\n",
    "    sns.stripplot(ax=axes[i], x=\"treatment\", y=var, data=grouped_df, dodge=False, c=\"black\", jitter=True, size=5)\n",
    "    axes[i].set_title(f\"Einfluss von Treatment auf {var}\")\n",
    "    axes[i].set_ylabel(var)\n",
    "\n",
    "# Layout anpassen, Plot anzeigen und speichern\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "PLOT_MANAGER.save_plot(fig, filename=\"effect_of_treatment_with_rope_release\", subdir=\"combined\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Für die betroffenen Parameter wird der Einfluss von `rope_release` mithilfe der bereits angepassten Modelle (`Parameter ~ C(treatment) + rope_release`) entfernt. Dazu werden Vorhersagen für einen konstanten `rope_release`-Wert (den Mittelwert) berechnet und mit den tatsächlichen Werten verglichen. Die daraus resultierenden bereinigten Werte sind frei von Variation, die auf `rope_release` zurückzuführen wäre."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grouped_adjusted_df = grouped_df.copy()\n",
    "\n",
    "# Bereinigung: Wir setzen rope_release auf seinen Mittelwert\n",
    "rope_mean = grouped_df[\"rope_release\"].mean()\n",
    "\n",
    "for var in relevant_vars_rope_release:\n",
    "    # Zugehöriges Modellobjekt abrufen\n",
    "    model = results[var][\"model\"]\n",
    "\n",
    "    # Vorhersage mit tatsächlichen rope_release-Werten\n",
    "    predicted_current = model.predict(grouped_df)\n",
    "\n",
    "    # Vorhersage, wenn rope_release = rope_mean gesetzt wird\n",
    "    df_mean_rope = grouped_df.copy()\n",
    "    df_mean_rope[\"rope_release\"] = rope_mean\n",
    "    predicted_mean = model.predict(df_mean_rope)\n",
    "\n",
    "    # Angepasste Werte berechnen:\n",
    "    actual = grouped_df[var].values\n",
    "    adjusted = actual + (predicted_mean - predicted_current)\n",
    "    grouped_adjusted_df[var] = adjusted"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grouped_df.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grouped_adjusted_df.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Latex Tabelle Output\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# DataFrame mit den relevanten Spalten erstellen\n",
    "variables_latex = [\n",
    "    'm_amplitude', \n",
    "    'm_amplitude_2',\n",
    "    'initial_amplitude',\n",
    "    'damping_coeff', \n",
    "    'damping_ratio', \n",
    "    'frequency_damped', \n",
    "    'frequency_undamped',\n",
    "    'pearson_r',\n",
    "    'nmae'\n",
    "]\n",
    "df_latex = grouped_adjusted_df[variables_latex + ['treatment']].copy()\n",
    "df_latex.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Spaltennamen entsprechend der LaTeX-Notation umbenennen\n",
    "df_latex.rename(columns=data_dict, inplace=True)\n",
    "\n",
    "# Erstellung deskriptiver Statistiken für alle Beobachtungen\n",
    "overall_stats = df_latex.describe().drop(index=['count', '25%', '75%'])\n",
    "overall_stats.rename(index={'50%': 'median'}, inplace=True)\n",
    "\n",
    "# Erstellung der deskriptiven Statistiken für jede Gruppe\n",
    "grouped_stats = {\n",
    "    'overall': overall_stats\n",
    "}\n",
    "\n",
    "for treatment, group in df_latex.groupby('treatment', observed=True):\n",
    "    group_stats = group.describe().drop(index=['count', '25%', '75%'])\n",
    "    group_stats.rename(index={'50%': 'median'}, inplace=True)\n",
    "    grouped_stats[treatment] = group_stats\n",
    "\n",
    "# Zusammenführen der Statistiken in einer Tabelle\n",
    "combined_stats = pd.concat(grouped_stats, names=['Treatment'])\n",
    "\n",
    "# LaTeX-Export des kombinierten DataFrames\n",
    "df_latex_string = combined_stats.to_latex(\n",
    "    escape=False,\n",
    "    multirow=True,\n",
    "    multicolumn=True,\n",
    "    column_format=\"l|lrrrrrrrrr\", \n",
    "    float_format=\"{:0.2f}\".format\n",
    ")\n",
    "\n",
    "# LaTeX-Tabellencode erstellen\n",
    "latex_table = f\"\"\"\n",
    "\\\\begin{{table}}[h]\n",
    "    \\\\centering\n",
    "    \\\\caption{{Feldversuch 2 - Ergebnisse, Schwingungsparameter deskriptive Statistiken (Gesamt und gruppiert über Treatment), Amplituden korrigiert über \\\\texttt{{rope\\\\_release}}, 9 Beobachtung je Gruppe, jeweils Mittelwert für 4 Elastometer}}\n",
    "    \\\\begin{{adjustbox}}{{max width=\\\\textwidth}}\n",
    "    {df_latex_string}\n",
    "    \\\\end{{adjustbox}}\n",
    "    \\\\label{{tab:Feldversuch_2_Deskriptive_Statistiken_Schwingungsparameter}}\n",
    "\\\\end{{table}}\n",
    "\"\"\"\n",
    "\n",
    "print(latex_table)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Visualisierung der bereinigten Ergebnisse\n",
    "\n",
    "Abschließend werden die bereinigten Werte grafisch dargestellt, um die Unterschiede zwischen den Behandlungen in Abwesenheit des `rope_release`-Einflusses zu verdeutlichen. Dies zeigt, wie sich die Treatments auf die Parameter auswirken würden, wenn für alle Einheiten die gleiche mittlere Vorspannung gelten würde."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "relevant_vars_rope_release = ['m_amplitude', 'm_amplitude_2', 'max_strain', 'max_compression']\n",
    "\n",
    "n_vars = len(relevant_vars_rope_release)\n",
    "n_cols = 2  # Links Original, rechts angepasst\n",
    "n_rows = n_vars  # Eine Zeile pro Variable\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(10, 4 * n_rows))\n",
    "\n",
    "for i, var in enumerate(relevant_vars_rope_release):\n",
    "    # Linke Spalte: Original (grouped_df)\n",
    "    sns.boxplot(ax=axes[i,0], x=\"treatment\", y=var, data=grouped_df, \n",
    "                palette=treatment_color_dict, hue=\"treatment\", legend=False, dodge=False)\n",
    "    sns.stripplot(ax=axes[i,0], x=\"treatment\", y=var, data=grouped_df, \n",
    "                  dodge=False, c=\"black\", jitter=True, size=5)\n",
    "    axes[i,0].set_title(f\"original: {var}\")\n",
    "    axes[i,0].set_ylabel(var)\n",
    "\n",
    "    # Rechte Spalte: Angepasst (grouped_adjusted_df)\n",
    "    sns.boxplot(ax=axes[i,1], x=\"treatment\", y=var, data=grouped_adjusted_df, \n",
    "                palette=treatment_color_dict, hue=\"treatment\", legend=False, dodge=False)\n",
    "    sns.stripplot(ax=axes[i,1], x=\"treatment\", y=var, data=grouped_adjusted_df, \n",
    "                  dodge=False, c=\"black\", jitter=True, size=5)\n",
    "    axes[i,1].set_title(f\"adjusted : {var}\")\n",
    "    axes[i,1].set_ylabel(var)\n",
    "    \n",
    "    # Y-Limits von links holen\n",
    "    y_min, y_max = axes[i,0].get_ylim()\n",
    "    # Y-Limits auf rechts anwenden\n",
    "    axes[i,1].set_ylim(y_min, y_max)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "PLOT_MANAGER.save_plot(fig, filename=\"effect_of_treatment_comparison\", subdir=\"combined\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Schritt 2: Durchführung von Post-hoc-Tests\n",
    "\n",
    "Festzustellen welche paarweisen Unterschiede zwischen den Treatments signifikant sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Wir gehen davon aus, dass grouped_adjusted_df existiert und die Spalten 'treatment' sowie die Variablen aus 'variables' enthält.\n",
    "\n",
    "for var in variables_spec:\n",
    "    # Tukey HSD Test durchführen\n",
    "    # Annahme: Die Spalte 'treatment' enthält die Gruppennamen z.B. 'free', 'gefa_dynamic', 'cobra_static'\n",
    "    # pairwise_tukeyhsd benötigt die abhängige Variable und die Gruppen.\n",
    "    tukey_results = pairwise_tukeyhsd(endog=grouped_adjusted_df[var],\n",
    "                                      groups=grouped_adjusted_df['treatment'],\n",
    "                                      alpha=0.05)\n",
    "    \n",
    "    print(f\"--- Post-Hoc Test (Tukey HSD) für Variable: {var} ---\")\n",
    "    print(tukey_results.summary())\n",
    "    print(\"\\n\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Anzahl der relevanten Variablen und Layout für die Subplots definieren\n",
    "n_vars, n_cols, n_rows = len(variables_spec), 2, (len(variables_spec) + 1) // 2\n",
    "\n",
    "# Subplots erstellen\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(10, 4 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Für jede relevante Variable einen Plot erstellen\n",
    "for i, var in enumerate(variables_spec):\n",
    "    sns.boxplot(ax=axes[i], x=\"treatment\", y=var, data=grouped_adjusted_df, palette=treatment_color_dict, hue=\"treatment\", legend=False, dodge=False)\n",
    "    sns.stripplot(ax=axes[i], x=\"treatment\", y=var, data=grouped_adjusted_df, dodge=False, c=\"black\", jitter=True, size=5)\n",
    "    axes[i].set_title(f\"Einfluss von Treatment auf {var}\")\n",
    "    axes[i].set_ylabel(var)\n",
    "\n",
    "# Layout anpassen, Plot anzeigen und speichern\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "PLOT_MANAGER.save_plot(fig, filename=\"effect_of_treatment_without_rope_release\", subdir=\"combined\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Anzahl der relevanten Variablen und Layout für die Subplots definieren\n",
    "n_vars, n_cols, n_rows = len(variables_spec), 2, (len(variables_spec) + 1) // 2\n",
    "\n",
    "# Subplots erstellen\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 5 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Für jede relevante Variable einen Plot erstellen\n",
    "for i, var in enumerate(variables_spec):\n",
    "    sns.boxplot(ax=axes[i], x=\"treatment\", y=var, data=grouped_adjusted_df, palette=treatment_color_dict, hue=\"treatment\", legend=False, dodge=False)\n",
    "    sns.stripplot(ax=axes[i], x=\"treatment\", y=var, data=grouped_adjusted_df, dodge=False, c=\"black\", jitter=True, size=5)\n",
    "    axes[i].set_title(f\"Einfluss von Treatment auf {var}\")\n",
    "    axes[i].set_ylabel(var)\n",
    "\n",
    "# Layout anpassen, Plot anzeigen und speichern\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "PLOT_MANAGER.save_plot(fig, filename=\"effect_of_treatment_without_rope_release_spec\", subdir=\"combined\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Vorhergesagte Werte extrahieren und Boxplots für die Sensoren erstellen"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vorhergesagte Werte aus den Modellen extrahieren\n",
    "for variable in variables:\n",
    "    df[f'predicted_{variable}'] = models[variable].fittedvalues\n",
    "\n",
    "# Boxplots erstellen mit den vorhergesagten Werten\n",
    "fig, axes = plt.subplots(4, 2, figsize=(8, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, variable in enumerate(variables):\n",
    "    sns.boxplot(ax=axes[i], x='treatment', y=f'predicted_{variable}', data=df, palette=treatment_color_dict, hue='treatment', dodge=False, legend=False)\n",
    "    axes[i].set_title(f'{variable} by Treatment')\n",
    "    axes[i].set_xlabel('Treatment')\n",
    "    axes[i].set_ylabel(f'Predicted {variable}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "PLOT_MANAGER.save_plot(fig, filename=\"predicted_effect_for_treatment\", subdir=\"combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def annotate_tukey(ax, tukey_result, significance_level=0.05):\n",
    "    \"\"\"\n",
    "    Fügt eine Textbox mit den Tukey-Test-Ergebnissen und dem festgelegten Signifikanzniveau in den Plot ein.\n",
    "    \n",
    "    Parameters:\n",
    "    ax (matplotlib.axes): Die Achse, auf der der Plot gezeichnet wird.\n",
    "    tukey_result (TukeyHSDResults): Die Ergebnisse des Tukey HSD Tests.\n",
    "    significance_level (float): Das Signifikanzniveau, standardmäßig 0.05.\n",
    "    \"\"\"\n",
    "    # Definiere die gewünschte Reihenfolge der Vergleiche\n",
    "    comparisons_order = [('free', 'gefa_dynamic'), ('free', 'cobra_static'), ('gefa_dynamic', 'cobra_static')]\n",
    "\n",
    "    # Text für die Annotation zusammenstellen\n",
    "    text_str = f\"Tukey HSD Results: \\n(Significance level = {significance_level:.2f})\\n\\n\"\n",
    "    \n",
    "    # Durchlaufe die gewünschte Vergleichsreihenfolge\n",
    "    for group1, group2 in comparisons_order:\n",
    "        # Filtere die korrekte Paarung aus den Tukey-Ergebnissen\n",
    "        for i in range(len(tukey_result._results_table.data[1:])):\n",
    "            pair = tukey_result._results_table.data[i + 1]\n",
    "            if (pair[0] == group1 and pair[1] == group2) or (pair[0] == group2 and pair[1] == group1):\n",
    "                p_value = tukey_result.pvalues[i]\n",
    "                significance = \"*\" if p_value < significance_level else \"n.s.\"\n",
    "                text_str += f\"\\n{group1} vs {group2}: \\np = {p_value:.4f} ({significance})\\n\\n\"\n",
    "    \n",
    "    # Textbox am Rand des Plots hinzufügen\n",
    "    ax.annotate(text_str, xy=(1.01, 0.1), xycoords='axes fraction', va='center', ha='left')\n",
    "\n",
    "# Einzelne Plots für jede Variable erstellen und speichern\n",
    "for variable in variables:\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    \n",
    "    # Boxplot für die aktuelle Variable\n",
    "    sns.boxplot(ax=ax, x='treatment', y=f'predicted_{variable}', data=df, \n",
    "                palette=treatment_color_dict, hue='treatment', dodge=False, legend=False)\n",
    "    \n",
    "    # Tukey-Test für die aktuelle Variable\n",
    "    tukey_result = tukey_results[variable]\n",
    "    \n",
    "    # Tukey-Ergebnisse annotieren\n",
    "    annotate_tukey(ax, tukey_result)\n",
    "    \n",
    "    ax.set_title(f'{variable} by Treatment')\n",
    "    ax.set_xlabel('Treatment')\n",
    "    ax.set_ylabel(f'Predicted {variable}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot speichern\n",
    "    plot_filename = f\"{variable}_effect_for_treatment\"\n",
    "    PLOT_MANAGER.save_plot(fig, filename=plot_filename, subdir=\"osc_variables_box\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gruppieren des DataFrames nach 'treatment' und Entfernen unnötiger Spalten\n",
    "df_treatment_describe = (df.drop(['id', 'ptq_sensor_name'], axis=1)\n",
    "                         .groupby('treatment', observed=True)\n",
    "                         .describe())\n",
    "\n",
    "df_treatment_describe = df_treatment_describe.reset_index()\n",
    "df_treatment_describe.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gruppieren des DataFrames nach 'treatment' und Entfernen unnötiger Spalten\n",
    "df_sensor = (df.drop(['id', 'release_force_target', 'ls3_rope_release', 'ls3_cable_max', 'location', 'height', 'diameter', 'direction'], axis=1).\n",
    "             groupby(['treatment', 'ptq_sensor_name'], observed=True).\n",
    "             mean())  #.T\n",
    "#df_sensor.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gruppieren des DataFrames und Anwenden von mean für ptq_sensor_name \n",
    "df_id = ((df.drop(['ptq_sensor_name', 'location', 'height', 'diameter', 'direction'], axis=1)\n",
    "          .groupby(['treatment', 'id'], observed=True)\n",
    "          .mean())\n",
    "         .reset_index())\n",
    "\n",
    "df_id.round(4).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Zusammenhangsanalyse für LS3 und PTQ"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Auswahl der neuen Spaltennamen für die Korrelationsmatrix\n",
    "columns_corr = ['ptq_m_amplitude',\n",
    "                'ptq_m_amplitude_2',\n",
    "                'ptq_initial_amplitude',\n",
    "                'ptq_damping_coeff',\n",
    "                'ptq_angular_frequency',\n",
    "                'ptq_y_shift',\n",
    "                'ptq_pearson_r',\n",
    "                #'ptq_nrmse',\n",
    "                'ptq_nmae',\n",
    "                'release_force_target',\n",
    "                'ls3_rope_release',\n",
    "                'ls3_cable_max']\n",
    "df_corr = df_id.copy()[columns_corr]\n",
    "\n",
    "# Berechnung der Korrelationsmatrix\n",
    "correlation_matrix = df_corr.corr()\n",
    "\n",
    "# Visualisierung der Korrelationsmatrix mit Seaborn\n",
    "fig1, ax = plt.subplots(figsize=(8, 8))  # Anpassen der Größe der Grafik\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', ax=ax, annot_kws={'size': 10})\n",
    "\n",
    "# Titel und Schriftgrößen anpassen\n",
    "#plt.title('Correlation Matrix for LS3 and PTQ', fontsize=18)\n",
    "ax.set_xticklabels(ax.get_xmajorticklabels(), fontsize=12, rotation=45, ha='right')\n",
    "ax.set_yticklabels(ax.get_ymajorticklabels(), fontsize=12, rotation=45, ha='right')\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "PLOT_MANAGER.save_plot(fig1, filename=\"correlation_matrix\", subdir=\"combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ANOVA für 'ls3_rope_release'\n",
    "model_rope_release = smf.ols('ls3_rope_release ~ treatment', data=df_id).fit()\n",
    "anova_rope_release = sm.stats.anova_lm(model_rope_release, typ=2)\n",
    "\n",
    "# ANOVA für 'ls3_cable_max'\n",
    "model_cable_max = smf.ols('ls3_cable_max ~ treatment', data=df_id).fit()\n",
    "anova_cable_max = sm.stats.anova_lm(model_cable_max, typ=2)\n",
    "\n",
    "# Zusammenfassungen der Modelle\n",
    "summary_rope_release = model_rope_release.summary()\n",
    "summary_cable_max = model_cable_max.summary()\n",
    "# Zusammenfassungen der Modelle\n",
    "summary_rope_release_latex = model_rope_release.summary().as_latex()\n",
    "summary_cable_max_latex = model_cable_max.summary().as_latex()\n",
    "\n",
    "#print(summary_rope_release_latex)\n",
    "#print(summary_cable_max_latex)\n",
    "\n",
    "anova_rope_release, summary_rope_release, anova_cable_max, summary_cable_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Funktion zur Erstellung des Text-Strings für die statistischen Parameter\n",
    "def annotate_stats(x, y):\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "    text_str = f\"R = {r_value:.2f}\\nSlope = {slope:.2f}\\nIntercept = {intercept:.2f}\\np-value = {p_value:.2e}\\nStd Err = {std_err:.2f}\"\n",
    "    return text_str"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Zusammenhang Ziel- und Ist-Vorspannung"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig2, ax1 = plt.subplots(figsize=(8, 5))\n",
    "sns.regplot(x='release_force_target', y='ls3_rope_release', data=df_id, ax=ax1, color='b', ci=95)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.axis('off')\n",
    "stats_text = annotate_stats(df_id['release_force_target'], df_id['ls3_rope_release'])\n",
    "ax2.annotate(stats_text, xy=(1.01, 0.1), xycoords='axes fraction')\n",
    "#ax1.set_title('Correlation Between Target and Actual Release Forces\"')\n",
    "ax1.set_xlabel('Release Force Target [kN]')\n",
    "ax1.set_ylabel('Release Force [kN]')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "PLOT_MANAGER.save_plot(fig2, filename=f\"release_force_target_vs_ls3_rope_release\", subdir=\"combined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Zusammenhang Vorspannung und resultierende Lastspitzen"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig3, ax1 = plt.subplots(figsize=(8, 5))\n",
    "ax2 = ax1.twinx()\n",
    "ax2.axis('off')\n",
    "y_pos_init = 0.1\n",
    "for idx, (treatment, color) in enumerate(treatment_color_dict.items()):\n",
    "    subset = df_id[df_id['treatment'] == treatment]\n",
    "    if subset['ls3_cable_max'].isna().all():\n",
    "        continue\n",
    "    sns.regplot(x='ls3_rope_release', y='ls3_cable_max', data=subset, ax=ax1, color=color, label=treatment, ci=95)\n",
    "    stats_text = annotate_stats(subset['ls3_rope_release'], subset['ls3_cable_max'])\n",
    "    ax2.annotate(f\"{treatment}:\\n{stats_text}\", xy=(1.01, y_pos_init + idx * 0.3), xycoords='axes fraction')\n",
    "#ax1.set_title('Correlation Between Release Force and Peak Cable Force')\n",
    "ax1.set_xlabel('Release Force [kN]')\n",
    "ax1.set_ylabel('Peak Force in Cable [kN]')\n",
    "ax1.legend(title='Treatment', loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "PLOT_MANAGER.save_plot(fig3, filename=f\"release_force_vs_ls3_cable_max\", subdir=\"combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig3, ax1 = plt.subplots(figsize=(8, 5))\n",
    "ax2 = ax1.twinx()\n",
    "ax2.axis('off')\n",
    "y_pos_init = 0.1\n",
    "for idx, (treatment, color) in enumerate(treatment_color_dict.items()):\n",
    "    subset = df_id[df_id['treatment'] == treatment]\n",
    "    if subset['ptq_m_amplitude'].isna().all():\n",
    "        continue\n",
    "    sns.regplot(x='ls3_rope_release', y='ptq_m_amplitude', data=subset, ax=ax1, color=color, label=treatment,\n",
    "                ci=95)\n",
    "    stats_text = annotate_stats(subset['ls3_rope_release'], subset['ptq_m_amplitude'])\n",
    "    ax2.annotate(f\"{treatment}:\\n{stats_text}\", xy=(1.01, y_pos_init + idx * 0.3), xycoords='axes fraction')\n",
    "#ax1.set_title('Correlation Between Release Force and Elongation Amplitude')\n",
    "ax1.set_xlabel('Release Force [kN]')\n",
    "ax1.set_ylabel('Elongation Amplitude [$\\mu$m] (mean for all Sensors)')\n",
    "ax1.legend(title='Treatment', loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "PLOT_MANAGER.save_plot(fig3, filename=f\"ls3_release_force_vs_ptq_m_amplitude\", subdir=\"combined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig3, ax1 = plt.subplots(figsize=(8, 5))\n",
    "ax2 = ax1.twinx()\n",
    "ax2.axis('off')\n",
    "y_pos_init = 0.1\n",
    "for idx, (treatment, color) in enumerate(treatment_color_dict.items()):\n",
    "    subset = df_id[df_id['treatment'] == treatment]\n",
    "    if subset['ptq_m_amplitude_2'].isna().all():\n",
    "        continue\n",
    "    sns.regplot(x='ls3_rope_release', y='ptq_m_amplitude_2', data=subset, ax=ax1, color=color, label=treatment,\n",
    "                ci=95)\n",
    "    stats_text = annotate_stats(subset['ls3_rope_release'], subset['ptq_m_amplitude_2'])\n",
    "    ax2.annotate(f\"{treatment}:\\n{stats_text}\", xy=(1.01, y_pos_init + idx * 0.3), xycoords='axes fraction')\n",
    "#ax1.set_title('Correlation Between Release Force and Elongation Amplitude 2')\n",
    "ax1.set_xlabel('Release Force [kN]')\n",
    "ax1.set_ylabel('Elongation Amplitude 2 [$\\mu$m] (mean for all Sensors)')\n",
    "ax1.legend(title='Treatment', loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "PLOT_MANAGER.save_plot(fig3, filename=f\"ls3_release_force_vs_ptq_m_amplitude_2\", subdir=\"combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Funktion zur Durchführung des ANOVA-Tests und Berechnung der Effektstärke (Eta Squared)\n",
    "def perform_anova_and_effect_size(df: pd.DataFrame, variable: str, treatments: List[str]) -> str:\n",
    "    groups = [df[df['treatment'] == treatment][variable].dropna() for treatment in treatments]\n",
    "    f_stat, p_value = f_oneway(*groups)\n",
    "\n",
    "    # Berechnung der Effektstärke (Eta Squared)\n",
    "    n = sum([len(g) for g in groups])\n",
    "    ss_total = sum([(x - df[variable].mean()) ** 2 for g in groups for x in g])\n",
    "    eta_squared = f_stat * len(groups) / (f_stat * len(groups) + (n - len(groups)))\n",
    "\n",
    "    # Überprüfung der Signifikanz\n",
    "    significance = \"*\" if p_value < 0.05 else \"\"\n",
    "\n",
    "    return f\"{variable}: {significance}\\nF-statistic = {f_stat:.2f}\\np-value = {p_value:.2e}\\nEta Squared = {eta_squared:.2f}\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Funktion zur Erstellung von Boxplots\n",
    "def create_boxplot(df: pd.DataFrame, variable: str, group_by: str, ax: plt.Axes, color_dict: Dict[str, str], perform_stats: bool) -> None:\n",
    "    valid_df = df.dropna(subset=[variable])\n",
    "    sns.boxplot(x=group_by, y=variable, hue=group_by, data=valid_df, ax=ax, palette=color_dict, dodge=False)\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.axis('off')\n",
    "    if perform_stats:\n",
    "        stats_text = perform_anova_and_effect_size(valid_df, variable, valid_df[group_by].unique())\n",
    "        ax2.annotate(stats_text, xy=(1.01, 0.1), xycoords='axes fraction')\n",
    "    ax.set_title(f'Einfluss von {group_by} auf {variable}')\n",
    "    ax.set_xlabel(group_by)\n",
    "    ax.set_ylabel(variable)\n",
    "\n",
    "# Funktion zur Erstellung kombinierter Plots\n",
    "def create_combined_plot(df: pd.DataFrame, columns: List[str], group_by: str, color_dict: Dict[str, str], num_columns: int = 3, perform_stats: bool = False) -> None:\n",
    "    num_rows = len(columns) // num_columns + (len(columns) % num_columns > 0)\n",
    "    fig, axes = plt.subplots(num_rows, num_columns, figsize=(16, 4 * num_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, variable in enumerate(columns):\n",
    "        create_boxplot(df, variable, group_by, axes[idx], color_dict, perform_stats)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    PLOT_MANAGER.save_plot(fig, filename=f\"combined_plot_{group_by}\", subdir=\"combined\")\n",
    "\n",
    "# Funktion zur Erstellung einzelner Plots\n",
    "def create_individual_plots(df: pd.DataFrame, columns: List[str], group_by: str, color_dict: Dict[str, str], perform_stats: bool = False) -> None:\n",
    "    for variable in columns:\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        create_boxplot(df, variable, group_by, ax, color_dict, perform_stats)\n",
    "        plt.tight_layout()\n",
    "        #plt.show()\n",
    "        PLOT_MANAGER.save_plot(fig, filename=f\"{group_by}_{variable}\", subdir=\"individual_plots\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = ['ptq_m_amplitude',\n",
    "           'ptq_m_amplitude_2',\n",
    "           'ptq_initial_amplitude',\n",
    "           'ptq_damping_coeff',\n",
    "           'ptq_angular_frequency',\n",
    "           'ptq_y_shift',\n",
    "           'ptq_pearson_r',\n",
    "           #'ptq_nrmse',\n",
    "           #'ptq_nmae',\n",
    "           #'release_force_target',\n",
    "           'ls3_rope_release',\n",
    "           'ls3_cable_max'\n",
    "           ]\n",
    "\n",
    "# Beispiel: Erstellen von Plots gruppiert nach 'treatment'\n",
    "create_combined_plot(df, columns, 'treatment', treatment_color_dict, perform_stats=True)\n",
    "create_individual_plots(df, columns, 'treatment', treatment_color_dict, perform_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Beispiel: Erstellen von Plots gruppiert nach 'ptq_sensor_name'\n",
    "columns = ['ptq_m_amplitude', 'ptq_m_amplitude_2', 'ptq_initial_amplitude', 'ptq_damping_coeff', 'ptq_angular_frequency', 'ptq_pearson_r']\n",
    "\n",
    "create_combined_plot(df, columns, 'ptq_sensor_name', sensor_color_dict)\n",
    "create_individual_plots(df, columns, 'ptq_sensor_name', sensor_color_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
